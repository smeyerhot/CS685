{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cABMCU7dsCDX",
    "outputId": "e3b9414e-14b4-46f4-e9a2-9f334fbda61f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc1zXZVCoeRd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/projects/og-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsQo8Lq6ssCo",
    "outputId": "3daf2a31-86b8-4f9a-ab97-b34b292fd367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from transformers) (0.0.15)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from transformers) (4.62.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp39-cp39-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 294 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 297 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.8.28-cp39-cp39-macosx_10_9_x86_64.whl (285 kB)\n",
      "\u001b[K     |████████████████████████████████| 285 kB 560 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Collecting click\n",
      "  Using cached click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: regex, click, tokenizers, sacremoses, pyyaml, transformers\n",
      "Successfully installed click-8.0.1 pyyaml-5.4.1 regex-2021.8.28 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n",
      "Collecting pytorch-pretrained-bert\n",
      "  Using cached pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from pytorch-pretrained-bert) (4.62.1)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from pytorch-pretrained-bert) (2.26.0)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from pytorch-pretrained-bert) (1.21.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from pytorch-pretrained-bert) (1.9.0)\n",
      "Requirement already satisfied: regex in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from pytorch-pretrained-bert) (2021.8.28)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.18.44-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 126 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.10.0.0)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.22.0,>=1.21.44\n",
      "  Downloading botocore-1.21.44-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 306 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.44->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.44->boto3->pytorch-pretrained-bert) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.44->boto3->pytorch-pretrained-bert) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/og-env/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
      "Successfully installed boto3-1.18.44 botocore-1.21.44 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fRmwhQwtYnD"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, GPT2LMHeadModel, GPT2DoubleHeadsModel, GPT2TokenizerFast, GPT2Config\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2DoubleHeadsModelOutput\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel,GPT2MultipleChoiceHead, GPT2Model, GPT2LMHead, Attention, Block, \\\n",
    "    LayerNorm, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "V2flo67Yt1ZZ",
    "outputId": "4794806c-3e28-494d-eca2-a0045c8b7177"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nTypes\\ntransformer_outputs => BaseModelOutputWithPastAndCrossAttentions\\nmodel => GPT2LMHeadModel > (something)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Types\n",
    "transformer_outputs => BaseModelOutputWithPastAndCrossAttentions\n",
    "model => GPT2LMHeadModel > (something)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gg8mKKRy7C4m",
    "outputId": "dcd69f43-6c4d-47b0-9904-710e70bd1945"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 7])\n",
      "tensor([[6, 6]])\n",
      "tensor([[8.8838, 8.6002]], grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([50258, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "\n",
    "# Add a [CLS] to the vocabulary (we should train it also!)\n",
    "num_added_tokens = tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
    "\n",
    "embedding_layer = model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size\n",
    "\n",
    "choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n",
    "encoded_choices = [tokenizer.encode(s) for s in choices]\n",
    "cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
    "\n",
    "input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
    "print(input_ids.size())\n",
    "mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
    "print(mc_token_ids)\n",
    "outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
    "lm_logits = outputs.logits\n",
    "mc_logits = outputs.mc_logits\n",
    "print(mc_logits)\n",
    "\n",
    "print(model.transformer.wte.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Te42AAn4eUk",
    "outputId": "1f3c8249-aa20-48ac-871c-76913bf25dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1],dtype=float)\n",
    "with torch.\n",
    "with torch.no_grad():\n",
    "  torch.set_grad_enabled(True)\n",
    "  x.requires_grad=True\n",
    "  print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0bmNP5sV90C",
    "outputId": "601a29b7-0f58-466b-cbe2-e2856c3f2e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([1, 2])\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,3)\n",
    "print(a)\n",
    "a = torch.transpose(a, 0, 1)\n",
    "b = torch.tensor([1,2])\n",
    "print(b)\n",
    "x =torch.multiply(b,a)\n",
    "print(torch.transpose(x, 0, 1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlSBxVvOs7Gh"
   },
   "outputs": [],
   "source": [
    "class CustomNet(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.interesting_head = GPT2LMHead(self.transformer.wte.weight, config)\n",
    "        self.funny_head = GPT2LMHead(self.transformer.wte.weight, config)\n",
    "        self.choosing_head = nn.Linear(self.transformer.wte.weight.size()[-1], 2)\n",
    "        self.choice_dispatch = {0: self.interesting_head, 1: self.funny_head}\n",
    "        self.rewards = {0: [], 1: []}  # max rewards hardcoded with even distribution of funny and interesting ie. 5:5\n",
    "        self.correct = {0: [], 1: []} \n",
    "        self.scores = {0: [], 1: []}\n",
    "        self.probs = {0: [], 1: []}\n",
    "        self.combined_logits = {0: [], 1: []}\n",
    "        self.logits = []\n",
    "        self.chosen_head = None\n",
    "        self.batch_size = 2\n",
    "        self.index = 0\n",
    "        self.choice = 0\n",
    "        self.generating = False\n",
    "        self.generation_step = -1\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.generation_step = -1\n",
    "        self.rewards = {0: [] , 1: []}  # max rewards hardcoded with even distribution of funny and interesting ie. 5:5\n",
    "        self.correct = {0: [], 1: []} \n",
    "        self.scores = {0: [], 1: []}\n",
    "        self.probs = {0: [], 1: []}\n",
    "        self.combined_logits = {0: [], 1: []}\n",
    "        self.stop_generation()\n",
    "        self.generation_step = -1\n",
    "\n",
    "    def affirm_social(self, correct):\n",
    "        if correct:\n",
    "            self.correct[self.choice].append(1)\n",
    "        else:    \n",
    "            self.correct[self.choice].append(0)\n",
    "\n",
    "    def affirm_reward(self, value):\n",
    "        self.rewards[self.choice].append(value)\n",
    "        \n",
    "    def start_generation(self):\n",
    "        self.generating = True\n",
    "\n",
    "    def stop_generation(self):\n",
    "        self.generating = False\n",
    "        self.generation_step = -1\n",
    "\n",
    "    def combine_logits(self):\n",
    "        logits = torch.stack(self.logits)\n",
    "        mean_logits = torch.mean(logits, dim=0).squeeze()\n",
    "        self.combined_logits[self.choice].append(mean_logits)\n",
    "        self.logits.clear()\n",
    "\n",
    "    def increment(self):\n",
    "        self.index += 1\n",
    "\n",
    "    def choose(self, sentence):\n",
    "        scores = F.relu(self.choosing_head(sentence))\n",
    "        probs = F.softmax(scores, dim=-1)\n",
    "        prob = torch.max(probs)\n",
    "        idx = torch.argmax(probs)\n",
    "        self.choice = idx.item()\n",
    "        self.probs[self.choice].append(prob.item())\n",
    "        self.scores[self.choice].append(scores)\n",
    "\n",
    "    def calculate_rewarded_loss(self):\n",
    "        loss = 0\n",
    "        for i in range(2):\n",
    "            combined = self.combined_logits[i]\n",
    "            if not len(combined):\n",
    "                loss += 0\n",
    "                continue\n",
    "\n",
    "            logits = torch.stack(combined, dim=0)\n",
    "            rewards = torch.tensor(self.rewards[i])\n",
    "            flipped_logits = torch.transpose(logits, 0, 1)\n",
    "            rewarded_logits = torch.multiply(flipped_logits, rewards)\n",
    "            rewarded_logits = torch.transpose(rewarded_logits, 0, 1)\n",
    "            scores = torch.mean(rewarded_logits, dim=0)\n",
    "            probs = F.softmax(scores, dim=-1)\n",
    "            prob = torch.max(probs)\n",
    "            print(f\"reward prob: {prob}\")\n",
    "            loss += -1 * torch.log(prob)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_cross_entropy(self):\n",
    "        loss = 0 \n",
    "        for i in range(2):    \n",
    "            iprobs = self.probs[i]\n",
    "            original_length = len(iprobs)\n",
    "            if not len(iprobs):\n",
    "                loss += 0\n",
    "                continue\n",
    "\n",
    "            probs = torch.tensor(iprobs)\n",
    "            mask = torch.tensor(self.correct[i], dtype=bool)\n",
    "            partial_loss = torch.sum(probs[mask], dim=0) / original_length\n",
    "            print(f\"choice prob: {partial_loss}\")\n",
    "            partial_loss = -1 * torch.log(partial_loss)\n",
    "            loss += partial_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_total_loss(self):\n",
    "       return self.calculate_rewarded_loss() + self.calculate_cross_entropy()\n",
    "\n",
    "    @torch.enable_grad()    \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            past_key_values=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            use_cache=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "        ):  \n",
    "      \n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = transformer_outputs.last_hidden_state\n",
    "        hidden_states = transformer_outputs.hidden_states\n",
    "        past = transformer_outputs.past_key_values\n",
    "        lm_logits = None\n",
    "        if self.generating:\n",
    "            lm_logits = self.chosen_head(last_hidden_state)\n",
    "            if self.generation_step > 0:\n",
    "                print(f\"lm_logits.size: {lm_logits.size()}\")\n",
    "                self.logits.append(lm_logits)\n",
    "        else:\n",
    "            self.chosen_head = self.choice_dispatch[self.choice]\n",
    "\n",
    "        loss = None\n",
    "        self.generation_step += 1\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=past,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdsOi9RPtbUk",
    "outputId": "76375f7d-1291-4deb-99f1-83a69da65005"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BudgieNet were not initialized from the model checkpoint at microsoft/DialoGPT-small and are newly initialized: ['interesting_head.decoder.weight', 'choosing_head.bias', 'funny_head.decoder.weight', 'choosing_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CustomNet.from_pretrained(\"microsoft/DialoGPT-small\", output_hidden_states = True)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlYMkmm2oODU"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZYzQy9YAr6C"
   },
   "outputs": [],
   "source": [
    "response_map = {1: \"funny\", 0:\"interesting\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jQI-ib4HeSCS",
    "outputId": "3a54f3c5-6ac4-4841-ace0-8d257e18e5ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Hello\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "tensor([[   40,  1101,   736,  5145, 50256]])\n",
      "DialoGPT: I'm back!\n",
      ">> Should I have been funny here ? - if not leave blank and enter1\n",
      ">> Was this actually funny ? - how much so 0-5 3\n",
      ">> User:What is new?\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "tensor([[ 2437,   338,   649,  5633, 50256]])\n",
      "DialoGPT: How's new?\n",
      ">> Should I have been funny here ? - if not leave blank and enteryes\n",
      ">> Was this actually funny ? - how much so 0-5 4\n",
      "Training the model\n",
      "reward prob: 0.5417230129241943\n",
      "choice loss: 0.966251015663147\n",
      "tensor(0.6473, grad_fn=<AddBackward0>)\n",
      ">> User:fsdfdsa\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "tensor([[ 2437,   649,  5633, 50256]])\n",
      "DialoGPT: How new?\n",
      ">> Should I have been interesting here ? - if not leave blank and enter3\n",
      ">> Was this actually interesting ? - how much so 0-5 3\n",
      ">> User:3\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "tensor([[ 2437,   649,  5633, 50256]])\n",
      "DialoGPT: How new?\n",
      ">> Should I have been interesting here ? - if not leave blank and enter3\n",
      ">> Was this actually interesting ? - how much so 0-5 3\n",
      "Training the model\n",
      "reward prob: 0.6869854927062988\n",
      "choice loss: 0.5\n",
      "tensor(1.0686, grad_fn=<AddBackward0>)\n",
      ">> User:3\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "lm_logits.size: torch.Size([1, 1, 50257])\n",
      "tensor([[ 3791,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           649,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           649,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           649,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           649,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           649,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           649,   649,   649,   649,   649,   649,   649,   649,   649,   649,\n",
      "           764, 50256]])\n",
      "DialoGPT: New new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new.\n",
      ">> Should I have been interesting here ? - if not leave blank and enterHello\n",
      ">> Was this actually interesting ? - how much so 0-5 f\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-0b0e768d1970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffirm_social\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0maccurate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">> Was this actually {response_map[model.choice]} ? - how much so 0-5 \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0maccurate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccurate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccurate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffirm_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccurate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'f'"
     ]
    }
   ],
   "source": [
    "model.reset_state()\n",
    "model.index = 0\n",
    "while model.index < 100:\n",
    "    while not (model.index % (model.batch_size + 1)) == 0:\n",
    "        context = input(\">> User:\")\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = tokenizer(context + tokenizer.eos_token, return_tensors='pt').input_ids\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if model.index > 1 else new_user_input_ids\n",
    "        output = model(bot_input_ids)\n",
    "        token_vecs = output.hidden_states[-2][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        model.choose(sentence_embedding)\n",
    "        model.start_generation()\n",
    "        # average the second to last hiden layer of each token to get sentence rep\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        chat_history_ids = model.generate(bot_input_ids, max_length=1000, return_dict=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        model.stop_generation()\n",
    "        model.combine_logits()\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(chat_history_ids[:, bot_input_ids.shape[-1]:])\n",
    "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "        model.increment()\n",
    "        social = input(f\">> Should I have been {response_map[model.choice]} here ? - if not leave blank and enter\")\n",
    "        model.affirm_social(len(social))\n",
    "        accurate = input(f\">> Was this actually {response_map[model.choice]} ? - how much so 0-5 \")\n",
    "        accurate = int(accurate) if len(accurate) else 0\n",
    "        model.affirm_reward(accurate)\n",
    "\n",
    "    if model.index != 0:\n",
    "        print(\"Training the model\")\n",
    "        optim.zero_grad()\n",
    "        loss = model.calculate_total_loss()\n",
    "        model.reset_state()\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    model.increment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvV3oK52689R",
    "outputId": "7fb80cfc-bdf1-41f6-f030-796175acbe83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġam', 'Ġheaded', 'Ġto', 'Ġthe', 'Ġbank', 'Ġto', 'Ġfish', '<|endoftext|>']\n",
      "['I', 'Ġam', 'Ġheaded', 'Ġto', 'Ġthe', 'Ġbank', 'Ġto', 'Ġget', 'Ġa', 'Ġloan', '<|endoftext|>']\n",
      "tensor([-0.0488,  0.0685,  0.1018,  0.0523, -0.1859], grad_fn=<SliceBackward>)\n",
      "tensor([ 0.0691, -0.0561,  0.0389, -0.0181, -0.0886], grad_fn=<SliceBackward>)\n",
      "tensor([ 9703, 22242, 32942,  3290,  8532,  6844, 21367, 39047, 40368, 41538,\n",
      "        30560, 26188, 17252, 15552, 32540, 19673, 25428, 30359, 24619, 43003])\n",
      "dogdogsDog dog Dog dogs DogshoundradorMagikarpcake puppy pets pupstakeslordsmanshipsburgcats canine\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/main_classes/model.html\n",
    "input_embs = model.get_input_embeddings()\n",
    "output_embs = model.get_output_embeddings()\n",
    "\n",
    "string1 = \"I am headed to the bank to fish\" + tokenizer.eos_token\n",
    "string2 = \"I am headed to the bank to get a loan\" + tokenizer.eos_token\n",
    "dog = \"dog\"\n",
    "tokens1 = tokenizer.tokenize(string1)\n",
    "tokens2 = tokenizer.tokenize(string2)\n",
    "dog_tokens = tokenizer.tokenize(dog)\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "# print(dog_tokens)\n",
    "ids1 = tokenizer(string1)['input_ids']\n",
    "ids2 = tokenizer(string2)['input_ids']\n",
    "ids3 = tokenizer(dog)['input_ids']\n",
    "# print(ids1)\n",
    "# print(ids2)\n",
    "# print(ids3)\n",
    "states1 = input_embs(torch.tensor(ids1, dtype=int))\n",
    "states2 = input_embs(torch.tensor(ids2, dtype=int))\n",
    "states3 = input_embs(torch.tensor(ids3, dtype=int))\n",
    "print(states1[-2][0:5])\n",
    "print(states2[-2][0:5])\n",
    "# print(states2)\n",
    "# print(states3)\n",
    "word = output_embs(states)\n",
    "tops = torch.topk(word, 20).indices\n",
    "print(tops)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tops)\n",
    "strings = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(strings)\n",
    "# Convert string to sequence of tokens (WRONG)\n",
    "# word = tokenizer.decode(14231)\n",
    "# print(word)\n",
    "# id = tokenizer.convert_tokens_to_ids(token)\n",
    "# string = tokenizer.convert_tokens_to_string(token)\n",
    "# token = tokenizer.convert_ids_to_tokens(id)\n",
    "\n",
    "\n",
    "# print(id, token,string)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BudgieChat.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
