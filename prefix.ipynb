{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "prefix.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed7924ba9ba74666b5f38a01e1274817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9b4008a038ca4c7eb6a40410d452af5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_98f50482725948e3a7c6f6566c462f2c",
              "IPY_MODEL_2d244a95a3d343f799ede895b87580fa",
              "IPY_MODEL_d2717b7012c24dfe928a86b99b684e61"
            ]
          }
        },
        "9b4008a038ca4c7eb6a40410d452af5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98f50482725948e3a7c6f6566c462f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cf93f5df07c648da9c41668c5418f903",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e28913d15d4943209ef5d5c7a020d492"
          }
        },
        "2d244a95a3d343f799ede895b87580fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_800080c188d94475b3688d15d7239374",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3489,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3489,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63ad463fe15943d09b90e7390d8fe373"
          }
        },
        "d2717b7012c24dfe928a86b99b684e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a2fb4467caf44609d20b137cf8f965b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 12.2k/? [00:00&lt;00:00, 287kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9006badbd09d4134a90092fd84dd7a66"
          }
        },
        "cf93f5df07c648da9c41668c5418f903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e28913d15d4943209ef5d5c7a020d492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "800080c188d94475b3688d15d7239374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63ad463fe15943d09b90e7390d8fe373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a2fb4467caf44609d20b137cf8f965b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9006badbd09d4134a90092fd84dd7a66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2998c7628334e68a8f3d3ecad1740b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37daa18e021d49d6bdebe45aeed05d2d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd4b53b3baa241079c329d136b2d0d36",
              "IPY_MODEL_5e22db4851324d8b8bf7cdfd9b6d8044",
              "IPY_MODEL_988ecd34a8444ba89a10cd7f4a4ab983"
            ]
          }
        },
        "37daa18e021d49d6bdebe45aeed05d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd4b53b3baa241079c329d136b2d0d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0ba4d47a9ba42b1b8ca7cacd52314f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23fde636092a4ccf94eb657c7f75f7c5"
          }
        },
        "5e22db4851324d8b8bf7cdfd9b6d8044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a36f54ae84ac4c97a4b1bd7027aa0b97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 805,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 805,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca4f633324484126991a3a34a4210026"
          }
        },
        "988ecd34a8444ba89a10cd7f4a4ab983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d19cab438a0432aa4f6f4e832fedfbb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.12k/? [00:00&lt;00:00, 42.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8583613553e4adfa3821a89184cb708"
          }
        },
        "b0ba4d47a9ba42b1b8ca7cacd52314f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23fde636092a4ccf94eb657c7f75f7c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a36f54ae84ac4c97a4b1bd7027aa0b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca4f633324484126991a3a34a4210026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d19cab438a0432aa4f6f4e832fedfbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8583613553e4adfa3821a89184cb708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "901a2568f19a44f098201136e9e9cb70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_529f17383ed046e3be7c68e64193aadd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f7760c1fd3a4b14aadf8d052b281ed0",
              "IPY_MODEL_ac0b4e29017b462b8eb66afd53a44bef",
              "IPY_MODEL_9b46e4de567c414aaebd23a23770880d"
            ]
          }
        },
        "529f17383ed046e3be7c68e64193aadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f7760c1fd3a4b14aadf8d052b281ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16c23f186b4a4724a4be019b30ec53d5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fb8d168dfe345c59077a7f311945c9f"
          }
        },
        "ac0b4e29017b462b8eb66afd53a44bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_28582bb7cfbe42d8b2ff16c22a7cb9da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_159371131e344af7b381a92ed16f1d9d"
          }
        },
        "9b46e4de567c414aaebd23a23770880d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fae0378254fe49c2a9b6d1f4f647ff6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/0 [00:00&lt;00:00,  5.36 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67ff810e2c714597900bc767fa0aa60a"
          }
        },
        "16c23f186b4a4724a4be019b30ec53d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fb8d168dfe345c59077a7f311945c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28582bb7cfbe42d8b2ff16c22a7cb9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "159371131e344af7b381a92ed16f1d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fae0378254fe49c2a9b6d1f4f647ff6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67ff810e2c714597900bc767fa0aa60a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b35f176a4444da4a26afeae9ee530cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b26df6cc7c1b4ebeb5cba2a2b2f789f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac216c27b2e6445cbc7492f975eada33",
              "IPY_MODEL_8a3725a4ca9a4426beffc1e998823c38",
              "IPY_MODEL_4e059da51d864d41b6fb9bed351330ba"
            ]
          }
        },
        "b26df6cc7c1b4ebeb5cba2a2b2f789f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac216c27b2e6445cbc7492f975eada33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8cc578462d104c34a2b612df04fe9154",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4832bdb4c1f0433680e8c43c0d51f8eb"
          }
        },
        "8a3725a4ca9a4426beffc1e998823c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_872a52610471472cbd965891fecfd667",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31d340971d6b4b59989bb61f07720ec7"
          }
        },
        "4e059da51d864d41b6fb9bed351330ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e381d7ea000743ed89c3c37367c07c2a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00, 20.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aeceb7addbf6453bbf7572272bef1c7a"
          }
        },
        "8cc578462d104c34a2b612df04fe9154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4832bdb4c1f0433680e8c43c0d51f8eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "872a52610471472cbd965891fecfd667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31d340971d6b4b59989bb61f07720ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e381d7ea000743ed89c3c37367c07c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aeceb7addbf6453bbf7572272bef1c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f83eea5df0b7439eaaa5477b0a9048bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_066b06ca0b294f11bcc4efda636fe2ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0de3db4dc15144bca2de7a49708dcdea",
              "IPY_MODEL_1f1d0fa56932484388216fecffdd0b5d",
              "IPY_MODEL_3d63d423c72347b0b6f6c78b3586e162"
            ]
          }
        },
        "066b06ca0b294f11bcc4efda636fe2ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0de3db4dc15144bca2de7a49708dcdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dd1617c027f54afb903b704a763be435",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_722f4161b4884d09a302966d10e268c4"
          }
        },
        "1f1d0fa56932484388216fecffdd0b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ceac95f8a0bc4545a3cb62515d222105",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 641,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 641,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee115c68ac6e44b4b673db92fbb031ba"
          }
        },
        "3d63d423c72347b0b6f6c78b3586e162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5e300847cfc849c68d4a195c90d3277f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 641/641 [00:00&lt;00:00, 15.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33b9d58b07174e8398eeec5e9e27d0fd"
          }
        },
        "dd1617c027f54afb903b704a763be435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "722f4161b4884d09a302966d10e268c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ceac95f8a0bc4545a3cb62515d222105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee115c68ac6e44b4b673db92fbb031ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e300847cfc849c68d4a195c90d3277f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33b9d58b07174e8398eeec5e9e27d0fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19d045ad54104a76bb034d5e76c9159d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ab219d793d0049598fa18d43690deaf3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f57d9dd8db234ff6a8f9eaff5d1d6701",
              "IPY_MODEL_5ee1632d84dc44968f667a4bdf84ef56",
              "IPY_MODEL_ff7d76d688434b5d800b364a47ddc7e5"
            ]
          }
        },
        "ab219d793d0049598fa18d43690deaf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f57d9dd8db234ff6a8f9eaff5d1d6701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_05b46de07c2b4200b1d6095aa8408b94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_294e0815dcc442d1befc27527d20922e"
          }
        },
        "5ee1632d84dc44968f667a4bdf84ef56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b6fe40b651a94f3592ffeb73763bc3b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11eb6675563d40539e523a5eecdb4a60"
          }
        },
        "ff7d76d688434b5d800b364a47ddc7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_968eaf72f6a7490cb22c5db9f933d8ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 1.13MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff0107752227466cb39b1eb53b164424"
          }
        },
        "05b46de07c2b4200b1d6095aa8408b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "294e0815dcc442d1befc27527d20922e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6fe40b651a94f3592ffeb73763bc3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11eb6675563d40539e523a5eecdb4a60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "968eaf72f6a7490cb22c5db9f933d8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff0107752227466cb39b1eb53b164424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a644a38d6c894ac48046d33851a14ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a002999088be4bd593b7440bfeea28a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5276c3efa4f545dcaa6505a450df4437",
              "IPY_MODEL_a58e9359a38146a698d855978a212290",
              "IPY_MODEL_cb4144231ad5454f8ce8c5c86969ea92"
            ]
          }
        },
        "a002999088be4bd593b7440bfeea28a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5276c3efa4f545dcaa6505a450df4437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bc1ab70932cb4ea188b23e63a424cc27",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_559c3a8e31a34a43891d37d979607d8d"
          }
        },
        "a58e9359a38146a698d855978a212290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6e5a1d44a384f0dbd174afe02e3992f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba3a1428974f4f5e884334784266ad3e"
          }
        },
        "cb4144231ad5454f8ce8c5c86969ea92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c7c263087f44220b59a7fc3467f7842",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 701kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f9923f1ea1e47ac8bdf5644cac93534"
          }
        },
        "bc1ab70932cb4ea188b23e63a424cc27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "559c3a8e31a34a43891d37d979607d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6e5a1d44a384f0dbd174afe02e3992f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba3a1428974f4f5e884334784266ad3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c7c263087f44220b59a7fc3467f7842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f9923f1ea1e47ac8bdf5644cac93534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c886a95e31340c0a171efdf6509bb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d7d61fc294f4973848db9ef236989d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_423e92d5219949ce94ab3dc93deeb0e4",
              "IPY_MODEL_9ceb6a80bb5742838986db390efcfeaa",
              "IPY_MODEL_626696c39c754aa485ad8602e8be5d7e"
            ]
          }
        },
        "7d7d61fc294f4973848db9ef236989d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "423e92d5219949ce94ab3dc93deeb0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_832ea0c197d741a9866f6c5df8270805",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_905e5029e9804de1b8ecbdc5bc0a2dab"
          }
        },
        "9ceb6a80bb5742838986db390efcfeaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e274b3a9ac147b0ade1c5a05a5e14d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73a0d957b107448b8a2a858d318509fe"
          }
        },
        "626696c39c754aa485ad8602e8be5d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28489e5fdf5a4a9887534db05fcf4d84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26.0/26.0 [00:00&lt;00:00, 588B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37ec8c8534f54c5983ce28f7a5470795"
          }
        },
        "832ea0c197d741a9866f6c5df8270805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "905e5029e9804de1b8ecbdc5bc0a2dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e274b3a9ac147b0ade1c5a05a5e14d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73a0d957b107448b8a2a858d318509fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28489e5fdf5a4a9887534db05fcf4d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37ec8c8534f54c5983ce28f7a5470795": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c460f3b2c9a542d3b2e45feebc955b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_48d68f183b1245958d97489605c42222",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0780d9c65b6b4287bfd2ade1d6144f81",
              "IPY_MODEL_a9af6453ff1f4cb4be30938fce0c068b",
              "IPY_MODEL_29e37f69bd554e63a36193f17f5e7dbc"
            ]
          }
        },
        "48d68f183b1245958d97489605c42222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0780d9c65b6b4287bfd2ade1d6144f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46e05f9ae5f54e158f31d39bfa74e756",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_452ae988ddf1458d98651676e0b2552d"
          }
        },
        "a9af6453ff1f4cb4be30938fce0c068b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6ce809e9df4d4eb3a6e2a64e6f284a23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 351265583,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 351265583,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17a96838625940b39bf2e8166ae43a8f"
          }
        },
        "29e37f69bd554e63a36193f17f5e7dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e3cf80e6d5a5414a9a54792b81777398",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 335M/335M [00:13&lt;00:00, 28.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2cd529da11b744e8bd45156c0e4ef7b9"
          }
        },
        "46e05f9ae5f54e158f31d39bfa74e756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "452ae988ddf1458d98651676e0b2552d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ce809e9df4d4eb3a6e2a64e6f284a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17a96838625940b39bf2e8166ae43a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3cf80e6d5a5414a9a54792b81777398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2cd529da11b744e8bd45156c0e4ef7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJRXZJd7Fc-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b08352-8a07-43f9-bc74-acd6f71a4675"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFQhxqyAFeE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e51bc83-8d4a-4131-b7e3-8b15ac804f7b"
      },
      "source": [
        "# !git clone https://github.com/kteavery/transformers.git\n",
        "# %pwd \n",
        "# %cd transformers\n",
        "!pip install git+https://github.com/kteavery/transformers.git\n",
        "\n",
        "# !pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pytorch_pretrained_bert\n",
        "\n",
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/kteavery/transformers.git\n",
            "  Cloning https://github.com/kteavery/transformers.git to /tmp/pip-req-build-h08lxo77\n",
            "  Running command git clone -q https://github.com/kteavery/transformers.git /tmp/pip-req-build-h08lxo77\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 29.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 446 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3277289 sha256=0f763103ae44fd8e73b1f4a1955a7f99b3cffc2da355c5b4d15e3f6002c269ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h69ynis6/wheels/5c/6f/aa/8f3f015ceb7aaef1c732ad3f606d4d97c70222c503b4870189\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 42.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.22-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.0.2)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.22\n",
            "  Downloading botocore-1.23.22-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.22->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.22->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.22 botocore-1.23.22 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n",
            "\u001b[31mERROR: Invalid requirement: 'rouge/requirements.txt'\n",
            "Hint: It looks like a path. File 'rouge/requirements.txt' does not exist.\u001b[0m\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOglUtjNXXc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bd09f7-e865-4da9-96c6-7e66a2f07f81"
      },
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPHKlzSVsWJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c3f89e-4fb0-40dc-dd23-4ff3147d6501"
      },
      "source": [
        "import os\n",
        "import time\n",
        "# from google.colab import drive\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# import transformers.src.transformers as transformers\n",
        "# from transformers.src.transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
        "# from transformers.src.transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
        "# from transformers.src.transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "\n",
        "# from transformers.src.transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from datasets import load_dataset\n",
        "from helper import format_time, rouge\n",
        "from data import GPT2Dataset, preprocessing, split_data\n",
        "from plots import loss_curves\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT0Yc_OmsWJP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184,
          "referenced_widgets": [
            "ed7924ba9ba74666b5f38a01e1274817",
            "9b4008a038ca4c7eb6a40410d452af5d",
            "98f50482725948e3a7c6f6566c462f2c",
            "2d244a95a3d343f799ede895b87580fa",
            "d2717b7012c24dfe928a86b99b684e61",
            "cf93f5df07c648da9c41668c5418f903",
            "e28913d15d4943209ef5d5c7a020d492",
            "800080c188d94475b3688d15d7239374",
            "63ad463fe15943d09b90e7390d8fe373",
            "9a2fb4467caf44609d20b137cf8f965b",
            "9006badbd09d4134a90092fd84dd7a66",
            "f2998c7628334e68a8f3d3ecad1740b1",
            "37daa18e021d49d6bdebe45aeed05d2d",
            "fd4b53b3baa241079c329d136b2d0d36",
            "5e22db4851324d8b8bf7cdfd9b6d8044",
            "988ecd34a8444ba89a10cd7f4a4ab983",
            "b0ba4d47a9ba42b1b8ca7cacd52314f0",
            "23fde636092a4ccf94eb657c7f75f7c5",
            "a36f54ae84ac4c97a4b1bd7027aa0b97",
            "ca4f633324484126991a3a34a4210026",
            "1d19cab438a0432aa4f6f4e832fedfbb",
            "d8583613553e4adfa3821a89184cb708",
            "901a2568f19a44f098201136e9e9cb70",
            "529f17383ed046e3be7c68e64193aadd",
            "8f7760c1fd3a4b14aadf8d052b281ed0",
            "ac0b4e29017b462b8eb66afd53a44bef",
            "9b46e4de567c414aaebd23a23770880d",
            "16c23f186b4a4724a4be019b30ec53d5",
            "1fb8d168dfe345c59077a7f311945c9f",
            "28582bb7cfbe42d8b2ff16c22a7cb9da",
            "159371131e344af7b381a92ed16f1d9d",
            "fae0378254fe49c2a9b6d1f4f647ff6b",
            "67ff810e2c714597900bc767fa0aa60a",
            "0b35f176a4444da4a26afeae9ee530cc",
            "b26df6cc7c1b4ebeb5cba2a2b2f789f3",
            "ac216c27b2e6445cbc7492f975eada33",
            "8a3725a4ca9a4426beffc1e998823c38",
            "4e059da51d864d41b6fb9bed351330ba",
            "8cc578462d104c34a2b612df04fe9154",
            "4832bdb4c1f0433680e8c43c0d51f8eb",
            "872a52610471472cbd965891fecfd667",
            "31d340971d6b4b59989bb61f07720ec7",
            "e381d7ea000743ed89c3c37367c07c2a",
            "aeceb7addbf6453bbf7572272bef1c7a"
          ]
        },
        "outputId": "77767d94-a434-42d2-85ff-d4143bf8abde"
      },
      "source": [
        "covid_dialog = load_dataset(\"covid_qa_ucsd\", \"en\", data_dir=\"/content/drive/MyDrive/CS685\" )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed7924ba9ba74666b5f38a01e1274817",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.49k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2998c7628334e68a8f3d3ecad1740b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/805 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration en-c080136eb0615511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset covid_qa_ucsd/en (download: Unknown size, generated: 473.58 KiB, post-processed: Unknown size, total: 473.58 KiB) to /root/.cache/huggingface/datasets/covid_qa_ucsd/en-c080136eb0615511/1.0.0/2a15b6e8fdc7cee91951d8f20ac2b26ede79fbef988919fbde22dbb97bf4df81...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "901a2568f19a44f098201136e9e9cb70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset covid_qa_ucsd downloaded and prepared to /root/.cache/huggingface/datasets/covid_qa_ucsd/en-c080136eb0615511/1.0.0/2a15b6e8fdc7cee91951d8f20ac2b26ede79fbef988919fbde22dbb97bf4df81. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b35f176a4444da4a26afeae9ee530cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT4pD4vdsWJQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "22bdb900-1f3a-457e-c61e-6bbbc90e9613"
      },
      "source": [
        "cd_df = covid_dialog['train'].to_pandas()\n",
        "print(len(cd_df))\n",
        "cd_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "572\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialogue_id</th>\n",
              "      <th>dialogue_url</th>\n",
              "      <th>dialogue_turns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>https://www.icliniq.com/qa/covid-19/i-have-cou...</td>\n",
              "      <td>{'speaker': [0, 1, 0, 1], 'utterance': ['I hav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>https://www.icliniq.com/qa/covid-19/i-have-a-l...</td>\n",
              "      <td>{'speaker': [0, 1, 0, 1, 0, 1, 0, 1], 'utteran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>https://www.icliniq.com/qa/coronavirus/can-cor...</td>\n",
              "      <td>{'speaker': [0, 1], 'utterance': ['Can Coronav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>https://www.icliniq.com/qa/covid-19/i-have-chi...</td>\n",
              "      <td>{'speaker': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>https://www.healthcaremagic.com/premiumquestio...</td>\n",
              "      <td>{'speaker': [0, 1], 'utterance': ['Does COVID-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dialogue_id  ...                                     dialogue_turns\n",
              "0            1  ...  {'speaker': [0, 1, 0, 1], 'utterance': ['I hav...\n",
              "1            2  ...  {'speaker': [0, 1, 0, 1, 0, 1, 0, 1], 'utteran...\n",
              "2            3  ...  {'speaker': [0, 1], 'utterance': ['Can Coronav...\n",
              "3            4  ...  {'speaker': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, ...\n",
              "4            5  ...  {'speaker': [0, 1], 'utterance': ['Does COVID-...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ia7GGJsWJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "c694a58a-89fe-49d9-d499-fa71136e2908"
      },
      "source": [
        "qa_df = preprocessing(cd_df)\n",
        "text = qa_df.text.copy()\n",
        "\n",
        "qa_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/data.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  all_fields = np.transpose(np.array(all_fields).reshape((4, -1)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>encoder</th>\n",
              "      <th>decoder</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[I have cough with no travel history. Is this ...</td>\n",
              "      <td>[Hello, I understand your concern. I just have...</td>\n",
              "      <td>[I have cough with no travel history. Is this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[I have cough with no travel history. Is this ...</td>\n",
              "      <td>[Hi, I would recommend you take n-acetylcystei...</td>\n",
              "      <td>[I have cough with no travel history. Is this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[I have a little fever with no history of fore...</td>\n",
              "      <td>[Hello, I can understand your concern.In my op...</td>\n",
              "      <td>[I have a little fever with no history of fore...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[I have a little fever with no history of fore...</td>\n",
              "      <td>[Hi, yes, upload in this query only. I will se...</td>\n",
              "      <td>[I have a little fever with no history of fore...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[I have a little fever with no history of fore...</td>\n",
              "      <td>[Hi, I can understand your concern. I have gon...</td>\n",
              "      <td>[I have a little fever with no history of fore...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id  ...                                               text\n",
              "0  0  ...  [I have cough with no travel history. Is this ...\n",
              "1  0  ...  [I have cough with no travel history. Is this ...\n",
              "2  1  ...  [I have a little fever with no history of fore...\n",
              "3  1  ...  [I have a little fever with no history of fore...\n",
              "4  1  ...  [I have a little fever with no history of fore...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ANiO5wAsWJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "6ee6c709-91f0-4b1c-a440-427bfc4d9513"
      },
      "source": [
        "doc_lengths = []\n",
        "\n",
        "for bio in text:\n",
        "  \n",
        "    # get rough token count distribution\n",
        "    tokens = nltk.word_tokenize(bio[0])\n",
        "\n",
        "    doc_lengths.append(len(tokens))\n",
        "\n",
        "doc_lengths = np.array(doc_lengths)\n",
        "\n",
        "sns.distplot(doc_lengths)\n",
        "             "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f540eed0f90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bkb6_V9sWJS"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oUBg0POsWJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823,
          "referenced_widgets": [
            "f83eea5df0b7439eaaa5477b0a9048bb",
            "066b06ca0b294f11bcc4efda636fe2ee",
            "0de3db4dc15144bca2de7a49708dcdea",
            "1f1d0fa56932484388216fecffdd0b5d",
            "3d63d423c72347b0b6f6c78b3586e162",
            "dd1617c027f54afb903b704a763be435",
            "722f4161b4884d09a302966d10e268c4",
            "ceac95f8a0bc4545a3cb62515d222105",
            "ee115c68ac6e44b4b673db92fbb031ba",
            "5e300847cfc849c68d4a195c90d3277f",
            "33b9d58b07174e8398eeec5e9e27d0fd",
            "19d045ad54104a76bb034d5e76c9159d",
            "ab219d793d0049598fa18d43690deaf3",
            "f57d9dd8db234ff6a8f9eaff5d1d6701",
            "5ee1632d84dc44968f667a4bdf84ef56",
            "ff7d76d688434b5d800b364a47ddc7e5",
            "05b46de07c2b4200b1d6095aa8408b94",
            "294e0815dcc442d1befc27527d20922e",
            "b6fe40b651a94f3592ffeb73763bc3b3",
            "11eb6675563d40539e523a5eecdb4a60",
            "968eaf72f6a7490cb22c5db9f933d8ab",
            "ff0107752227466cb39b1eb53b164424",
            "a644a38d6c894ac48046d33851a14ca7",
            "a002999088be4bd593b7440bfeea28a7",
            "5276c3efa4f545dcaa6505a450df4437",
            "a58e9359a38146a698d855978a212290",
            "cb4144231ad5454f8ce8c5c86969ea92",
            "bc1ab70932cb4ea188b23e63a424cc27",
            "559c3a8e31a34a43891d37d979607d8d",
            "e6e5a1d44a384f0dbd174afe02e3992f",
            "ba3a1428974f4f5e884334784266ad3e",
            "3c7c263087f44220b59a7fc3467f7842",
            "3f9923f1ea1e47ac8bdf5644cac93534",
            "9c886a95e31340c0a171efdf6509bb48",
            "7d7d61fc294f4973848db9ef236989d2",
            "423e92d5219949ce94ab3dc93deeb0e4",
            "9ceb6a80bb5742838986db390efcfeaa",
            "626696c39c754aa485ad8602e8be5d7e",
            "832ea0c197d741a9866f6c5df8270805",
            "905e5029e9804de1b8ecbdc5bc0a2dab",
            "4e274b3a9ac147b0ade1c5a05a5e14d9",
            "73a0d957b107448b8a2a858d318509fe",
            "28489e5fdf5a4a9887534db05fcf4d84",
            "37ec8c8534f54c5983ce28f7a5470795",
            "c460f3b2c9a542d3b2e45feebc955b98",
            "48d68f183b1245958d97489605c42222",
            "0780d9c65b6b4287bfd2ade1d6144f81",
            "a9af6453ff1f4cb4be30938fce0c068b",
            "29e37f69bd554e63a36193f17f5e7dbc",
            "46e05f9ae5f54e158f31d39bfa74e756",
            "452ae988ddf1458d98651676e0b2552d",
            "6ce809e9df4d4eb3a6e2a64e6f284a23",
            "17a96838625940b39bf2e8166ae43a8f",
            "e3cf80e6d5a5414a9a54792b81777398",
            "2cd529da11b744e8bd45156c0e4ef7b9"
          ]
        },
        "outputId": "99db8638-a0fc-4054-9982-b72610c34944"
      },
      "source": [
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
        "config = GPT2Config.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-small\", config=config)\n",
        "print(model.config)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f83eea5df0b7439eaaa5477b0a9048bb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19d045ad54104a76bb034d5e76c9159d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a644a38d6c894ac48046d33851a14ca7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c886a95e31340c0a171efdf6509bb48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c460f3b2c9a542d3b2e45feebc955b98",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"_name_or_path\": \"microsoft/DialoGPT-small\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd9QBcC5sWJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba19cabf-8b9f-4c4f-8579-f305cfb0648f"
      },
      "source": [
        "batch_size = 2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "unique_vals = qa_df.id[len(qa_df)-1]\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.8 * unique_vals)\n",
        "val_size = int(0.1 * unique_vals)\n",
        "test_size = unique_vals - train_size - val_size\n",
        "\n",
        "train_split, val_split, test_split = split_data(qa_df, [train_size, val_size, test_size])\n",
        "\n",
        "train_split.reset_index(drop=True, inplace=True)\n",
        "val_split.reset_index(drop=True, inplace=True)\n",
        "test_split.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(train_split.head())\n",
        "\n",
        "print(\"length of dataset: \" + str(unique_vals))\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))\n",
        "\n",
        "train_dataset = GPT2Dataset(train_split, tokenizer, max_length=768)\n",
        "val_dataset = GPT2Dataset(val_split, tokenizer, max_length=768)\n",
        "test_dataset = GPT2Dataset(test_split, tokenizer, max_length=768)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    id  ...                                               text\n",
            "0  515  ...  [I travelled to Mauritius and do not have symp...\n",
            "1  304  ...  [Because I have type 1 diabetes, what is likel...\n",
            "2  329  ...  [I was confirmed to have covid-19, I self-isol...\n",
            "3  180  ...  [Good morning I have sore throat for 2 days no...\n",
            "4  313  ...  [I've been having the same symptoms as for Cor...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "length of dataset: 571\n",
            "  456 training samples\n",
            "   57 validation samples\n",
            "   58 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoGq0BvAsWJV"
      },
      "source": [
        "# Create the DataLoaders for our training and validation datasets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNAYzpcRsWJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0816d271-6ff8-4839-ea5e-169c9bcd0e03"
      },
      "source": [
        "useCuda = True\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "print(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2-2BFjsWJW"
      },
      "source": [
        "# I'm not really doing anything with the config buheret\n",
        "# configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# instantiate the model\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
        "# otherwise the tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "# if torch.cuda.is_available() and useCuda:\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBBw-4-zsWJX"
      },
      "source": [
        "# some parameters I cooked up that work reasonably well\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_every = 100\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z_W8s1ssWJX"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCrJbmGBTGqC"
      },
      "source": [
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "# This changes the learning rate as the training loop progresses\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLUgsXPtsWJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3650c0f-6c64-40a1-d30e-1cae0257eb94"
      },
      "source": [
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[5].to(device)\n",
        "        b_labels = batch[5].to(device)\n",
        "        b_masks = batch[6].to(device)\n",
        "        b_prefix_masks = batch[3].to(device)\n",
        "        # print(b_input_ids)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          prefix_mask = b_prefix_masks,\n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    # bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    min_length = 50,\n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[5].to(device)\n",
        "        b_labels = batch[5].to(device)\n",
        "        b_masks = batch[6].to(device)\n",
        "        b_prefix_masks = batch[3].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = model(b_input_ids, \n",
        "                             prefix_mask = b_prefix_masks,\n",
        "                             attention_mask = b_masks,\n",
        "                             labels=b_labels)\n",
        "          \n",
        "            loss = outputs[0]  \n",
        "\n",
        "            if step % 2 == 0 and not step == 0:\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "              sample_outputs = model.generate(\n",
        "                                    # bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    min_length = 50,\n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "              for i, sample_output in enumerate(sample_outputs):\n",
        "                    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     1  of    228. Loss: 5.290461540222168.   Elapsed: 0:00:02.\n",
            "0: ...............................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     2  of    228. Loss: 5.6141743659973145.   Elapsed: 0:00:04.\n",
            "0: .......,.,.,,,.,..,,,,,,..,.,.,.,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     3  of    228. Loss: 3.8802638053894043.   Elapsed: 0:00:06.\n",
            "0: ,..,,,,.,,?,,?.,....,?,.,.,.?,,.,.,..,.,.,.,....,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     4  of    228. Loss: 4.2494988441467285.   Elapsed: 0:00:07.\n",
            "0: .,,..,......,.?........?,......,,., '....,.!!,.,,....?,...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of    228. Loss: 7.989079475402832.   Elapsed: 0:00:09.\n",
            "0: 2ch RES RES nsfw bing link 1 j bb RES u jst 3 2 yay 3u 3 3?? i dont really even know but RES can do everything here with some help. EDIT : not sure if RES can do that, but yeah.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     6  of    228. Loss: 3.044290065765381.   Elapsed: 0:00:11.\n",
            "0: 2ch F1 SNSR KAT DNB 3k 3k DNL 4.jp F amp F KSP B5f 2z2k F amp F KSP 6.0. 0 amp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp amp nbsp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     7  of    228. Loss: 2.1413116455078125.   Elapsed: 0:00:14.\n",
            "0: NSRl DRO 3 2 2 30 r 2 2 2 1 15 r 3 2 1 2 3 3 4 30 r 3 1 1 1 30 6 3 5 1 x 1 x 2 x x 0 x 2 3 1 1 5 r 3 x 1 x 1 x 2 x 2 x 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     8  of    228. Loss: 2.41884183883667.   Elapsed: 0:00:17.\n",
            "0: DV Fw 1000 SVF AT SLK SVF EZ 1 M SVF 5 SVF 7 SVF FW 1 Fw 0ff0f 0f 0f0f0f 0f 10 20 Fw 0f00 10f50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     9  of    228. Loss: 2.185551166534424.   Elapsed: 0:00:19.\n",
            "0: ,.,,,..,,.,?,?...,....,???,.,,,.?,.,,,?,,?,,.,.,.,.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of    228. Loss: 4.288002967834473.   Elapsed: 0:00:21.\n",
            "0: 30 5 15 20 60 20 30 4 5 5 5 20 3 1 2 3 1 2 5 30 6 5 10 30 5 15 20 30 3 3 30 5 10 15 3 10 30 35 5 30 4 5 3 30 3 5 30 4 45 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    11  of    228. Loss: 1.7716245651245117.   Elapsed: 0:00:22.\n",
            "0: 2ch SNS VB SNS PM 2ch SNS PM SNS 3 SNS YSK SNS SP 5 SNS SP NS 2ch SNS YSK SNS SNS 3 SNS YSK Edit : SNS I\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    12  of    228. Loss: 2.0304412841796875.   Elapsed: 0:00:24.\n",
            "0: 1 100 100 1000 1 10 1 200 1 1 200 1 500 1 30 1 1 100 1 250 1000 1000 100 1 200 1 200 50 100 100 100 100 30 30 50 100 1 100 20 I'm so happy I have you RES here for a week\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    13  of    228. Loss: 1.6779959201812744.   Elapsed: 0:00:26.\n",
            "0: Google,,....,.,..,.......,.!.,.!,....!..,..?...!....,.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    14  of    228. Loss: 2.86989688873291.   Elapsed: 0:00:28.\n",
            "0: .?.,...?...?........?!!!!!!!!!!!!!! 1,..... 1? 1? 1?!!?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    15  of    228. Loss: 2.9398467540740967.   Elapsed: 0:00:30.\n",
            "0: The correct answer would have been something like, we could use some of these for the new and improved BUD 9. So instead of a BUD 8 it should be an A A E in this case, as it is an in between.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    16  of    228. Loss: 1.685241937637329.   Elapsed: 0:00:32.\n",
            "0: 5 2 3 1 100 3 3 2 1 1 1 3 100 3 1 1 1 3 100 50 1 1 1 1 100 100 100 100 100 50 50 60 100 100 60 35 30 1 1 1 1 1 1 100 100 5 3 3 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    17  of    228. Loss: 2.163978099822998.   Elapsed: 0:00:34.\n",
            "0: ?,,,,,,,,, 10, 20's you, the video was a while after it was uploaded and he also had the video with the time in the video that he was just like... you could have watched that\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    18  of    228. Loss: 2.3626275062561035.   Elapsed: 0:00:36.\n",
            "0: ? I don't get it... what is it that I'm missing? I can't see anything about the quote I quoted it from to understand it, but it would be more like I was my life is like the light to me if I'm that person.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    19  of    228. Loss: 1.297764778137207.   Elapsed: 0:00:38.\n",
            "0: ...,.,.,.?, 10.,.,. 10..!!!,,., 10. 10... and my first in the middle or left if I'm taking a line in my last comment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    20  of    228. Loss: 1.9292171001434326.   Elapsed: 0:00:40.\n",
            "0: ,.. 10.. 20.. 20!. 20... 10 30.. 10 10. 10 20 20. 20 30 90.8 20.4,20,100,000,000.24.000,9.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    21  of    228. Loss: 1.9443591833114624.   Elapsed: 0:00:42.\n",
            "0: That's a lot of chugs. You need to have a go with a lot of chugs and or a lower one for it to be as easy as the two do the body's first and the other's first chug, to get the desired length of the chug.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    22  of    228. Loss: 1.4118680953979492.   Elapsed: 0:00:44.\n",
            "0: Dv W2.txt W2.txt W2.txt W2.txt w2.txt w2.txt W2.txt.txt W2.txt.wav W2.txt W2.txt.wav W2.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    23  of    228. Loss: 1.695923924446106.   Elapsed: 0:00:46.\n",
            "0: .,..,..,.,, :,.,,., 2. 3 3. 3,5,,,.. 2,, 3.5. I think the number you are using is a bit off.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    24  of    228. Loss: 2.3987560272216797.   Elapsed: 0:00:48.\n",
            "0: .,,,.,.., :P a. a for a brief sentence and the last two are in a zig for you can't do. If that's what you meant by the sentence or a to get a sentence to end on and\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    25  of    228. Loss: 2.112415313720703.   Elapsed: 0:00:50.\n",
            "0: ,,. ',,'and. It can be done with an and or for a full table in the text but you know it from the back of the page it's just a weird syntax. It should be in the text body of the text or the back of the text body\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    26  of    228. Loss: 1.55567467212677.   Elapsed: 0:00:52.\n",
            "0: ?...,..,.,...,..,.,.,,.,.,.,,,. 20 20,2,3.21, 19,20.25 20.2.5 10.19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    27  of    228. Loss: 2.1725687980651855.   Elapsed: 0:00:54.\n",
            "0: .,.?..,.?.......?.....? 14 20,18 19,19.19,19,19.19,19.19.19.19.19,19,19.19,19.19,19,19,19,19.19.19,19,19.19.19.19,19.19,19.19,19.19,19,19.19.19.19.19,19.19,19.19.19,19.19,19.19,19.19,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    28  of    228. Loss: 1.934444546699524.   Elapsed: 0:00:58.\n",
            "0: ., 10 30 250.. 20 10 20,,,.. 30 20 250. 21,.40. 30.60.. 2:. 20, 30.80.,.80.1 20.20, 8.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    29  of    228. Loss: 2.241332769393921.   Elapsed: 0:00:59.\n",
            "0: ,.. I know you are being sarcastic but your sentence makes me think you are having a go at the user and a lot of the other comments to use as a question to you would make the same exact question. You should just have a short answer in your text and a few days of text to text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    30  of    228. Loss: 1.208479881286621.   Elapsed: 0:01:02.\n",
            "0: , a.facebook.com.tbs.u.chilid.q.aol.aol.com.chilid.com.on.my.facebook.com.in or to.facebook.com.en.on.on.com.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    31  of    228. Loss: 2.470372200012207.   Elapsed: 0:01:04.\n",
            "0: ... and the answer to your next question can you be a little bit more specific like your questions about the time at which a phone was the last of the days and the day at which you were the last of the days is a text from the last of the days and the answer is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    32  of    228. Loss: 1.1987510919570923.   Elapsed: 0:01:06.\n",
            "0: ,.,.....,.,,.,.,,. or. you know,,,., the the I you should do the right and you have to go to your doctor to get a test next to your doctor's or this\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    33  of    228. Loss: 1.119616985321045.   Elapsed: 0:01:08.\n",
            "0: 4,3 4,3 4,2 8,2 14.15.20,19,19.19.20,19.20.20,20.19.20,1920,20,1920.20.1919.19,20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    34  of    228. Loss: 2.9707629680633545.   Elapsed: 0:01:10.\n",
            "0: , 10.? 10. a. s for the answer to you have to your question as well or an a or is a text at the next to the user when asked for it or when you have the answer for a year or so as the answer was there but someone will know what you are talking about\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    35  of    228. Loss: 1.644526481628418.   Elapsed: 0:01:12.\n",
            "0: ? and you will know you have a better one when you have to know you've been around me and you know your old and have to know you're still around you'll be a better and it will probably take the rest of you a few days to talk to me and get there.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    36  of    228. Loss: 1.6742570400238037.   Elapsed: 0:01:14.\n",
            "0: .,,,......,.. 10,15 20.. 10 15.20 15,20 15.30,30. 15,10 20,20,20,20 or 20,20,20,30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    37  of    228. Loss: 1.3245735168457031.   Elapsed: 0:01:16.\n",
            "0: ... s.r.lyc.b.h.c.m.l.p.r.l.p.c.c.c. a.p.l.ny.b.p.p.r.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    38  of    228. Loss: 1.3561323881149292.   Elapsed: 0:01:18.\n",
            "0: ,....,, to: to: to: and the day of the doctor's to: Friday 19:00 to: 20:00 to: Thursday 19:30, in: to: to the day of the Doctor and to:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    39  of    228. Loss: 1.5681182146072388.   Elapsed: 0:01:20.\n",
            "0: 2ch 15 2:1:25:18.5:19:25.0.19.23.3,0.3.0.19.20.0.0.19.21.19.19.21.2.19.1919.8.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    40  of    228. Loss: 1.264114260673523.   Elapsed: 0:01:22.\n",
            "0: 10 10 7,9 7,4 2,7 for now.In a week you will get one and you can do it to for the first time since the 3 days or you will get a text from the next week and a text from a year or so or your last text for the day this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    41  of    228. Loss: 1.7538937330245972.   Elapsed: 0:01:24.\n",
            "0: DV R for Dv or or a VD V or DV for a VD or DV or a VD or a VD The VD V for VE V is not a VD V is a V V to V D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    42  of    228. Loss: 1.7191039323806763.   Elapsed: 0:01:26.\n",
            "0: ,  ile.mov and ile.mov.zip for your video video. video.mp3 is like a video out of the video.avid to the iphone iphone id  ile.mov.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    43  of    228. Loss: 2.8446037769317627.   Elapsed: 0:01:28.\n",
            "0: , text and text are two very very important and important text and text text. and text text is text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    44  of    228. Loss: 1.5776301622390747.   Elapsed: 0:01:33.\n",
            "0: MOD 3changbong chat.chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.Chat.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    45  of    228. Loss: 0.9900795221328735.   Elapsed: 0:01:36.\n",
            "0: .??? 19 20 20,19 21,19 19,19 19,19 19,19 21,19 19,19 19,19 19,19 19,19 19,19 19,19 19,19 19,19 21 19,1919 19,19 21,19 19,19 19,19 19,19 191919 19,19 19,191919 19,1919 1919 19,1919,1919 19,191919 19,1919,1919 19,19,19 19,19,19,19 19,1919 19,1919 19,19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    46  of    228. Loss: 1.1969412565231323.   Elapsed: 0:01:39.\n",
            "0: .., 19:19:19:19,19:19,19:1919:1919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    47  of    228. Loss: 1.1672160625457764.   Elapsed: 0:01:44.\n",
            "0: .? Is the answer in the text post text post text or text text text post text post text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    48  of    228. Loss: 0.896226167678833.   Elapsed: 0:01:48.\n",
            "0: .... 19,19,19,191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    49  of    228. Loss: 0.8761280179023743.   Elapsed: 0:01:53.\n",
            "0: DVF JW is the next for the JW in the family to take a short and short to follow as they come up with the next JW and follow the next one up and then another JW and the next and so on and so on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    50  of    228. Loss: 1.381339430809021.   Elapsed: 0:01:55.\n",
            "0: ,                                                                                                                             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    51  of    228. Loss: 0.9334609508514404.   Elapsed: 0:01:58.\n",
            "0: 5:1:5:3:3:5:3:3:3: 4:1:3:5:3:3:3:5,3,3:3:3:3:3,3,3:2:3:3:3,3,3:1:3,3,3,3:3:3:3,3,3:3,3,3,3,3,3,3,3,3,3,3:3,3,3,3,1:1:3,1,3,3,3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    52  of    228. Loss: 1.5420434474945068.   Elapsed: 0:02:02.\n",
            "0: ?                                                                                                                             a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    53  of    228. Loss: 1.187430739402771.   Elapsed: 0:02:05.\n",
            "0: , ian                                                                                                                          ,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    54  of    228. Loss: 1.3567795753479004.   Elapsed: 0:02:09.\n",
            "0: ..?......,... but the video is not very video and not a video video of any video of any video video and then video text and text to video and video text video video video text video video video video video video video text and text video text video video video video and text video video video video video text text video text video video text video video text video video text video text video video text text text text text text video text text video text text video video text text video text text video text text text text text text text text text text text text text text text text text text text text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    55  of    228. Loss: 1.9973748922348022.   Elapsed: 0:02:12.\n",
            "0: ,?? or should be to be. Is this what you like to do with your text chat on chat.net chat or chat.in chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatChat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    56  of    228. Loss: 1.1393085718154907.   Elapsed: 0:02:17.\n",
            "0: ,                                                                                                                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    57  of    228. Loss: 0.7585654258728027.   Elapsed: 0:02:20.\n",
            "0: ,                                                                                                                             !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    58  of    228. Loss: 0.9008192420005798.   Elapsed: 0:02:24.\n",
            "0: ,,! 19,2,1919,19191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919451919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    59  of    228. Loss: 0.831963300704956.   Elapsed: 0:02:29.\n",
            "0: 1 7,3 7,3 7,3.5 7,3,4,3,5 7,5,5,5,5,5,5,5,5,5,5,5,5,5,5,3,6,5 7,5,5,5,5,5,5,5,5,5,5,3,5,5,1,4,3,5,5,4,5,5,5,1,6,5,5,3,5,5,0,5,4,3,5,6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    60  of    228. Loss: 1.1661297082901.   Elapsed: 0:02:32.\n",
            "0: , is a better way to do it than text or video text text chat chat chat chat text chat text chat text chat text chat chat chat chat chat chat chat chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chattwitter chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    61  of    228. Loss: 0.9676563739776611.   Elapsed: 0:02:37.\n",
            "0: ?                                                                                                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    62  of    228. Loss: 1.311146855354309.   Elapsed: 0:02:40.\n",
            "0: ,, 2,,?., 2,?,. 3,:D:D:D:D, D:D, 4,1,19,19:,19:19,19:1919,1919,191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    63  of    228. Loss: 1.424062728881836.   Elapsed: 0:02:45.\n",
            "0: :P:p:p:p:m.wcg.us.fnc.k.fncw.fnc.scot.us.scot.m.nnc.qc.a.qc.c.s.i.qc.c: nc.m.cc.t.m.zc.s.m.lc.cbcbc.cbc.wcbc.sbcbc.cbc?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    64  of    228. Loss: 1.3236913681030273.   Elapsed: 0:02:48.\n",
            "0: ..                                                                                                                         !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    65  of    228. Loss: 0.7613869309425354.   Elapsed: 0:02:52.\n",
            "0: 20,500,000 people or less and or what is this now a country in the world like most of people do this but are a country or is like a nation of people who do this or do it because you like what they do do and what the most and can do what they do out of their text chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chattext chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    66  of    228. Loss: 0.6391100883483887.   Elapsed: 0:02:57.\n",
            "0: ...? Do you have this on video now for video chat with chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    67  of    228. Loss: 1.3627586364746094.   Elapsed: 0:03:00.\n",
            "0: .                                                                                                                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    68  of    228. Loss: 2.2997758388519287.   Elapsed: 0:03:03.\n",
            "0: ?                                                                                                                           !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    69  of    228. Loss: 1.509190559387207.   Elapsed: 0:03:07.\n",
            "0: , and you just text chat on text chat. chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat Chat chat chat, text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    70  of    228. Loss: 0.7252339720726013.   Elapsed: 0:03:12.\n",
            "0: : o: l:: o:? o:? : o:?  o:? o: o:  am I out of touch or did text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    71  of    228. Loss: 1.2277332544326782.   Elapsed: 0:03:17.\n",
            "0: ,                                                                                                                          ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    72  of    228. Loss: 0.6414117813110352.   Elapsed: 0:03:20.\n",
            "0: edit: O P M P P M P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P P\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    73  of    228. Loss: 1.9709153175354004.   Elapsed: 0:03:24.\n",
            "0: ?                                                                                                                        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    74  of    228. Loss: 2.3264684677124023.   Elapsed: 0:03:27.\n",
            "0: ?                                                                                                   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    75  of    228. Loss: 0.976051926612854.   Elapsed: 0:03:30.\n",
            "0: 1,2,3,5,6,10,15,15,16,20,20,19,19,19,19,19,20,19,19,19,20,19,19,19,19,19,19,19,19,19,19,1919,19,1919,19,1919,19,19,19,1919,19,19,19,19,1919,1919,1919,19,19,19,19,19,19,19,19,19,19,19,1919,1919,19,19,1919,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,1919,19,19,19,191919,19,19,19,19,19,1919,19,19,19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    76  of    228. Loss: 0.8824108839035034.   Elapsed: 0:03:35.\n",
            "0: ? but but but but but what about the text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, chat.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    77  of    228. Loss: 1.1641712188720703.   Elapsed: 0:03:38.\n",
            "0: ?                                                                                                                      ,    Hi.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    78  of    228. Loss: 1.2070801258087158.   Elapsed: 0:03:42.\n",
            "0: , 2:1,2,3,5,6, 7,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    79  of    228. Loss: 1.13164222240448.   Elapsed: 0:03:45.\n",
            "0: , text chat text chat text chat text chat chat chat text chat text chat chat text chat chat text chat chat text chat text chat chat chat text chat text chat chat text chat text chat text chat chat chat chat chat text chat chat chat chat text chat chat chat chat chat chat text text text text text text text chat chat text text text text text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    80  of    228. Loss: 1.8388516902923584.   Elapsed: 0:03:48.\n",
            "0: ,, video chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, text chat chat chat chat chat chat text, textChat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    81  of    228. Loss: 0.9058278203010559.   Elapsed: 0:03:52.\n",
            "0: . 1 4,400, 4,500 10,800 days,4,600,00,00,00,600 4 days,600,00,00,600,00,00,00,600 4 days,600,00,00,00,600,00,00,00 days,00,600,00,00 days.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    82  of    228. Loss: 0.490776002407074.   Elapsed: 0:03:55.\n",
            "0: ?                                                                                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    83  of    228. Loss: 0.4038723409175873.   Elapsed: 0:03:57.\n",
            "0: , 4:19,1919,191919,19191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191919191918191919In brief19, then the text chat with the text chat with me  Your text chat with yourself and text chat chat with me on the text chat with me on text chat chat with yourself on text chat with chat with chat with chat with your text chat with chat with text chat with text chat with text chat with me you are text chat with text chat with chat with me\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    84  of    228. Loss: 1.1050385236740112.   Elapsed: 0:04:02.\n",
            "0: ,   ,                                                                                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    85  of    228. Loss: 0.8660789132118225.   Elapsed: 0:04:05.\n",
            "0: ?                                                                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    86  of    228. Loss: 0.9066019654273987.   Elapsed: 0:04:08.\n",
            "0: ’ ’’’’��,?”. ” A text chat.In text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatChat, chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    87  of    228. Loss: 0.9458193182945251.   Elapsed: 0:04:13.\n",
            "0: ,.....,        ,   ,   ,   ,  ,   ,   ,   ,    ,        ,    ,   ,    ,  ,      ,    ,           ,         ,  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    88  of    228. Loss: 0.9632530212402344.   Elapsed: 0:04:16.\n",
            "0: ???  I text chat with text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat textchat is chat chat chat,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    89  of    228. Loss: 1.057210922241211.   Elapsed: 0:04:20.\n",
            "0: , xpost from the current issue text chat text chat chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, text chat chat text chat! text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat text chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    90  of    228. Loss: 1.3439922332763672.   Elapsed: 0:04:24.\n",
            "0: ?                                                                                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    91  of    228. Loss: 1.1575214862823486.   Elapsed: 0:04:27.\n",
            "0: .... 4.. 4 4,500,000 7,500,000,000 4,500,000,000 4,500,000 4,500,000 4,500,000 4,500,000 4,500,000 4,500,000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    92  of    228. Loss: 3.0110740661621094.   Elapsed: 0:04:29.\n",
            "0: ?? text chat chat text chat chat chat text chat text chat chat chat chat chat text chat text chat chat chat chat chat chat chat text chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    93  of    228. Loss: 3.335236072540283.   Elapsed: 0:04:32.\n",
            "0: . chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat text text chat text chat chat text! chat chat chat chat chat chat! chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    94  of    228. Loss: 1.297980785369873.   Elapsed: 0:04:34.\n",
            "0: ?..???????????????????? What is your question to this text chat chat text chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatTalk chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    95  of    228. Loss: 2.2225241661071777.   Elapsed: 0:04:39.\n",
            "0: I am a very short person and this video is very brief and brief brief and brief and brief brief brief.In brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief short brief brief,Hello! helicoptered brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief brief\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    96  of    228. Loss: 2.4869046211242676.   Elapsed: 0:04:44.\n",
            "0: , and so the question is answered.In brief the question of whether or not video chat and text chat will be available in text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat?, or chat chat chat chatHi mathematololololol chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    97  of    228. Loss: 0.6143858432769775.   Elapsed: 0:04:49.\n",
            "0: ’ ’’��?�blog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblogblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.19 advoczbollah is a blog.nycblog,nycblog.nycblog.nycblog.nycblogblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblog.nycblogblog.nycblog.nycblogblog.nycblog.nycblog.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    98  of    228. Loss: 1.1815036535263062.   Elapsed: 0:04:54.\n",
            "0: ,,,...,,.,,,,.,.,,,.,,.,,, in the text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat helicop: helicopue chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    99  of    228. Loss: 1.1200600862503052.   Elapsed: 0:04:59.\n",
            "0: ,.,. Welcome to the subreddit the community is not the one we like to talk about or follow here for that is the community of people like to video or text chat chat chat with others to chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat! chat lawyel chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   100  of    228. Loss: 0.9872329831123352.   Elapsed: 0:05:03.\n",
            "0: ,... Welcome to the world you live in with people like these people   What country do you live in and what culture do you live in where people like to spread these kinds of propaganda against people like these people. Welcome to the world you live in then.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   101  of    228. Loss: 1.068803071975708.   Elapsed: 0:05:06.\n",
            "0: ’ ’ ’’?’’’, you can use the right side of this as a verb if it is right side is it not right side?’’’’, there are certain cases where it is appropriate to use a foreign right side, you can use the right side of the verb and a different verb to be honest about it is not right side of the right side of the right side in certain cases right side with me you know when is to use the right side of this question\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   102  of    228. Loss: 0.5817352533340454.   Elapsed: 0:05:09.\n",
            "0: I understand this very brief but very brief to do not understand the purpose of text chat and text chat or text chat with text chat. If this is not your goal or goal in text chat chat or text chat chat with text chat or text chat with text chat it is not clear to me.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   103  of    228. Loss: 1.5867981910705566.   Elapsed: 0:05:11.\n",
            "0: ,? 14,000 14,000 14,000 14,000 14,000 14,000 14,000 14,000,000 14,000,000 14,000 14,000 14,000 14,000 14,000 14,000 14,000,000,000 14,000,000 14,000 14,500,000 14,000 14,000,000 14,000,000 14,000,000 14,000,000 14,000,000 14,000,000 14,000 14,000,000 14,000,000 14,000,000,000,000, helicoptered.1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,001,000,000,000,000,000,000,000,00,000,000,000,000,000,00,000,000,000,00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   104  of    228. Loss: 1.048051357269287.   Elapsed: 0:05:16.\n",
            "0: . tm.tm.mmsm tm.mmsm tmm.mmsmmsmm.mmsmmsmmsm tmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmmsmsmsmmsmsmmsmmsmsmmsmmsmmsmsmsmmsmmsmmsmsmmsmmsmmsmmsmsmsmmsmsmsmsmmsmsmsmmsmsmsmsmmsmsmsmsmHello.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   105  of    228. Loss: 1.0253788232803345.   Elapsed: 0:05:19.\n",
            "0: ,,.,. 4 or, 2, or,4 or, 4 or 4,5 or,5,5,5,5 or 8,5 5,5 or 8,5 or 4,5.5 or 5,5 or 4,5 or 4,5 or 5,5 or 4,5 or 4,5 or 7,5 or 7,5 or 8,5 or 4,5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   106  of    228. Loss: 1.2695857286453247.   Elapsed: 0:05:22.\n",
            "0: 10 2,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,.5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   107  of    228. Loss: 1.0012012720108032.   Elapsed: 0:05:27.\n",
            "0: ’ ’ ’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’”’’””,“  ”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   108  of    228. Loss: 1.1452610492706299.   Elapsed: 0:05:30.\n",
            "0: ? is it just me or the video is a video of the video where the video also video or video and video and video with video or video with video or video video and video and video video video and video video and video video video and video? video video or video video\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   109  of    228. Loss: 1.1308255195617676.   Elapsed: 0:05:32.\n",
            "0: In brief video chat and text chat chat with me when text chat with me when text chat chat with text chat with text chat chat with text chat with text chat chat chat chat chat chat with text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat toile, chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   110  of    228. Loss: 0.9348236918449402.   Elapsed: 0:05:37.\n",
            "0: 100 100.0000 0.0000 0,0000 0.0000 0,0 x 1,00 2,03,03 0,03 3,3 3,13.0 x,3,13,0,3,0,0.0 x3,0,3,0,3,3,2,0,0,3,5,0,3,4.0,0,0,3,0,3,0,0,3,3,0,0,3,5,0,3,0,0,0,3,0,3,0,3:4,3,0,0,3,3,3,0,0,3,0.0,3,0.3,5,3,5,3,4,0,0,0,0,0,3,3,0,4,5,0,0,0,0.0,0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   111  of    228. Loss: 0.9301788210868835.   Elapsed: 0:05:42.\n",
            "0: ?                                                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   112  of    228. Loss: 0.47182053327560425.   Elapsed: 0:05:44.\n",
            "0: ????In brief:PFT.FTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTWhat is a TFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   113  of    228. Loss: 0.9132167100906372.   Elapsed: 0:05:49.\n",
            "0: ...... I can assist in taking down the video if needed video or text chat with a video chat with a video chat with a video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat video chat with video chat chat with video chat with video chat with video chat with video chat with video chat with video chat video chat video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat video chat video chat chat video chat!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   114  of    228. Loss: 0.9587199687957764.   Elapsed: 0:05:52.\n",
            "0: ???????In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:?In brief:In brief: brief:In brief:In brief:In brief: brief:In brief:In brief: brief: brief:In brief: brief: brief: brief: brief:In brief: brief: brief: brief:In brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   115  of    228. Loss: 0.4898240268230438.   Elapsed: 0:05:57.\n",
            "0: , and it will be hard to be a better person than this son of a foreign mafmafmafmafmafmafmafmafmafmafmafmafmafmafmafmafmafafmafmafmafmafmafafmafmafmafafmafmafmafafmafmafmafmafafmafafmafmafmafmafmafafmafafmafafmafmafmafmafmafafmafmafmafmafmaf’mafafmafmafafafafafmafafafafafmafmafmafmafmafmafafafafaffamafafafafmafafafamafafafaffafafafafafafafafmafafafafmafafmafafafafafaf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   116  of    228. Loss: 1.0678284168243408.   Elapsed: 0:06:02.\n",
            "0: , S. J EJY PYFIA OFI JYFIA JYGIAI IJAIAYBETI BET TOOL OFIDEN OFIG BETHIN HEDI IS OUT OF MY SEED ZERO SEED OUT OF MY SEED OF SEED IN MY SEED OF OFED TO LIKE MY SEED TO LIKE ME LIKE MY SEED OF SEED LIKE SEED OUT OF MY SEED OF SEED OUT OF SEED SEED SEED OUT OF MY SEED SEED SEED OUT OF SEED OF SEED OUT OF SEED OF SEED TO launces a video or text chat chat with me with me and text chat with me\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   117  of    228. Loss: 0.780228316783905.   Elapsed: 0:06:06.\n",
            "0: It is more about showing a video video chat with someone and video chat with them than text chat with text chat with video chat with someone and text chat with video chat with text chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat video chat video chat chat video with’ with video chat video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat with video chat chat with video chat with video chat chat with video chat with chat with video chat with video chat with chat video chat chat chat with chat with video chat with video chat with video chat with video chat chat with\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   118  of    228. Loss: 1.0225058794021606.   Elapsed: 0:06:11.\n",
            "0: , for as a text chat service chat with text chat chat text chat chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chattext chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   119  of    228. Loss: 1.5396641492843628.   Elapsed: 0:06:16.\n",
            "0: ????’.    Also  As a person with anxiety and as a person with anxiety and as a person with anxiety and as a person with anxiety and as a person with anxiety and as a person with anxiety and as a person with anxiety and as a person with anxiety?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   120  of    228. Loss: 1.1321640014648438.   Elapsed: 0:06:18.\n",
            "0: .  mavans are just like the spaceships from the movie but with more muscle and less muscle and less muscle mass and more muscle and less muscle mass, with more muscle mass and more muscle and less muscle and less muscle. Do not use them because they tend to not be happy to be seen as muscle and can be seen as a muscle in the video or text chat with me or text chat with me text chat with me text chat with me text chat chat with me text chat with me text chat chat with me text chat chat with me text chat with text chat with text chat with me text chat with text chat text chatThe text chat chat chat with me chat chat chat chat with text chat chat chat with text chat chat with me chat chat chat chat chat with text chat with me chat chat with text chat chat chat chat chat with text chat with me chat chat chat chat chat chat with me text chat chat with text chat chat with text chat with text chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   121  of    228. Loss: 0.9886914491653442.   Elapsed: 0:06:23.\n",
            "0: . Cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough cough, cough, cough cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough cough, cough, cough, cough, cough,laughs, cough, cough, cough cough, cough, cough, cough, cough cough, cough, cough, cough, cough, cough, cough, cough cough, cough, cough cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough, cough,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   122  of    228. Loss: 0.9033254981040955.   Elapsed: 0:06:28.\n",
            "0: ?  Why is this not on r me  Can someone pleez text chat chat with me about this  Does this video include video chat with me text chat with text chat chat with text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatchat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   123  of    228. Loss: 0.5523356199264526.   Elapsed: 0:06:32.\n",
            "0: ????In brief: video or text chat with me, it only text chat with text chat with me, text chat with me text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatChat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   124  of    228. Loss: 0.8312036395072937.   Elapsed: 0:06:37.\n",
            "0: ......?????,.????,.,. 14.. 14.14.15.14.14. 15 15.15.14.14 14.14 14.15,14.14.14,14.16, 14 14,16.14,15,14,14,14.14,14,15,14,15,14,14,14,14,14,16,15,14,14,14,14,15,15,15,14,14.14,14,14,14,14,14 They did I can be in a 14 15,14.14,14,14,14,14,14,15,14.15.15, 14,15,14,14,14,14,14.14,14,14,14,14,14,14,14,14,14,15,14, 14,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   125  of    228. Loss: 1.289160966873169.   Elapsed: 0:06:42.\n",
            "0: ,.... and chat with me with me? text chat chat text chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   126  of    228. Loss: 0.9595052599906921.   Elapsed: 0:06:47.\n",
            "0: ... lt 2 10 13.3 T T T T T T T T T t T T t T T T t t t T T t t T T t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t tzbollah t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   127  of    228. Loss: 0.6487880945205688.   Elapsed: 0:06:52.\n",
            "0: ?In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:, brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief:In brief: brief:In brief: brief:In brief: brief:In brief:In brief:In brief:In brief:In brief: brief:In brief:In brief: brief: brief: brief: brief: brief:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   128  of    228. Loss: 0.9577414989471436.   Elapsed: 0:06:57.\n",
            "0: , 2.5. 2.5. 5,5. 5,5. 5,5.5,5.5.5,5,5,5,5,5,5,5,5,5,5,5,5,5.5.5,5,5,5,5,5,5,5.5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,?5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   129  of    228. Loss: 0.871650218963623.   Elapsed: 0:07:02.\n",
            "0: , text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatChat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   130  of    228. Loss: 0.7283861637115479.   Elapsed: 0:07:06.\n",
            "0: . 5 6 13. 5 13 4,6 6,6.6 14:14 14 3.5 4,6.5 6,7 6,7 14 4,6 6,3.7 6,6 6,7 4,3 6,3.3 4,6 6,7 6 6,3,3,3,5 6.3 4,3,3 6,3,3,3.3 6.5 7,7 2,3 6,3 6,3,5 13:3,5 6,6,3 6,5 6,5 6,3,3interstitial.3,3 6,3,7,3,5,6,7,3,3,5 6,7,3,3,5,5,5,3,5,5 6,5,5,6,5,5,7,7.5,5,5,4,3,3,5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   131  of    228. Loss: 0.7926152944564819.   Elapsed: 0:07:11.\n",
            "0: D A S C Y C C H A S T A M A Y C A S S C E S T E M S C H E S C S S C A S E S S C T C H T E S C E S S M S A S C M A S S C H S M S S E S S T S E S M S C S S E S C S H S S S S S C S E S H S S S S S C A S S S C S S S S S E S S S S S S S T S S S E S T S S S CIn brief:D S S A S S S E S S S S S S S S T S S S S M E S S C S S C S S E S S S S C A S S C H S S S S T S S S S C S S S S S M E S S A S S S S S S E\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   132  of    228. Loss: 0.9765335321426392.   Elapsed: 0:07:16.\n",
            "0: ? on that wall it would have been on the curb not on video. text chat or text chat with me text chat chat text chat text chat text chat chat chat chat chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   133  of    228. Loss: 1.068494439125061.   Elapsed: 0:07:21.\n",
            "0: ??? Sry to help but my question is not answered in my text chat text chat text chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat deficai chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   134  of    228. Loss: 0.5893609523773193.   Elapsed: 0:07:26.\n",
            "0: .. on my side is my left side and my left side is my left side is my right side and is left side because my left side is my left side is right side is my left side because my left side is my right side is my left side is my left side is my left side because my left side is left side is my left side is my left side is my left side is my left side is my left side is my left side is my left side is my left side is my left side side on my left side is my left side is my left side is my left side is my left side on my left side?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   135  of    228. Loss: 0.5433370471000671.   Elapsed: 0:07:29.\n",
            "0: .                                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   136  of    228. Loss: 0.986847460269928.   Elapsed: 0:07:33.\n",
            "0: ’ I have got them all and it makes you both worse as a person that lacks them and lacks them and as a person with the same need for those to take care of others. No need to worry, just call to video or text chat with me and video chat with me and text chat with me text chat or text chat with me and text chat with me and text chat with text chat with me and text chat with me and text chat with text chat chat with me. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   137  of    228. Loss: 1.387438416481018.   Elapsed: 0:07:35.\n",
            "0: . 3 D T, T, T T, T T, T3 D, T S and T D D3 D:D T3 D3 CTS, CT DTSC, CT DTSD T DTS, T DTSD, T DTSD, T DTSD. T T DTSD, CT DTSD T DTSD, T DTSD, CT DTSD, T DTSD, T DTSD, T DTSD, T DTSD, T DTSD, TDTSD T DTSD, TTS DTSDTSD, TTS DTSDTS, TTSDTS DTS, TTS TTSDTSDTSDTS, TTSDTS DTSDTS TTS TTSDTS, SDSTSDTS DTSTTS DTSDTS, DTS TTS DTSDTS TTS DTS, TTS TTS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   138  of    228. Loss: 0.6830608248710632.   Elapsed: 0:07:40.\n",
            "0: ? I got chills when I started reading this. Welcome to the subreddit! You got them when I started taking my acid. Welcome to the sub to the sub, and welcome to video chat with me about the video chat chat with me about video chat with me about video chat with me about video chat with me about video chat with me.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   139  of    228. Loss: 1.0305904150009155.   Elapsed: 0:07:43.\n",
            "0: ?? Wut  Would you like to chat with me? text chat with me? text chat chat text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat, chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   140  of    228. Loss: 1.0681556463241577.   Elapsed: 0:07:47.\n",
            "0: I do understand where this is coming from but it lacks the video or text or text chat with me and text chat with me that makes me understand the video text chat video or video chat video chat video chat with text chat video chat video chat video chat video chat video chat video chat chat video chat video chat video chat video chat chat video chat video chat video chat video chat video chat chat chat chat chat chat chat chat video chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat captcha chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   141  of    228. Loss: 0.7625262141227722.   Elapsed: 0:07:52.\n",
            "0: ,,. and or text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   142  of    228. Loss: 0.8591142296791077.   Elapsed: 0:07:57.\n",
            "0: , 2 3 3, 7,0 13,6, 6,5 14,5,3,5 14,5,7,5 13,5 14,3,3,3,4 14,3 13,3 14,4,3 14,5 13,3,3 13,5 14,3,3 14,4 13,3,5 14,5 14,7,0,3 14,3 14,5 13,5 14,3,3,5 14,4 13,5 14,3,5 14,5 14,4 13,5 13,5 13,?15,3,13,3,5,3 14,13,5,3,5,3,5 14,5,3,3,14,5,3,3,5,5,3,5,5,3,5,5 14,5 14,5,5,15,6,5,5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   143  of    228. Loss: 1.3569133281707764.   Elapsed: 0:08:02.\n",
            "0: ’’’’’’’’’’ ’’’’’’’’’ ’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’�’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   144  of    228. Loss: 0.4133782982826233.   Elapsed: 0:08:07.\n",
            "0: ??In brief:PFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTTheFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFTFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   145  of    228. Loss: 0.7837942242622375.   Elapsed: 0:08:12.\n",
            "0: ’ ’ malaise’ is common in Africa and Indonesia with Mauritian in this group as part of this group. And Malaise is common worldwide with Mauritian Mauritius in Maurita is also Mauritius is a Mauritibolibolibolibolibolibolibolibibibolibolibibolibolibolibolibolibolibolibibolibolibibolibolibolibibolibolibolibolibibibolibibolibolibibibibibolibibolibolibol mathematimibibibolibolibolibibolibibolibolibolibibolibolibolibolibolibolibolibolibolibolibibolibibibolibolibolibibolibolibibibolibolibibibibolibolibol\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   146  of    228. Loss: 0.966378390789032.   Elapsed: 0:08:16.\n",
            "0: ,, 4.75 or 6.75 x WFT, or 5ft 6ft 2 4 ft 3 feet.5 feet 4 feet 4 feet or less in length, 4 feet 4 feet 2 feet 6 feet is 5 feet for 6 feet under 4 feet under 4 feet, or 5 feet under 6 feet 3 feet 6 feet 2 feet is 5 feet deep.5 feet 5 feet 6 feet 6 feet 6 feet under 4 feet 7 feet is a half feet deep in line and a half deep half deep in line.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   147  of    228. Loss: 1.7866131067276.   Elapsed: 0:08:20.\n",
            "0: ..  Avoid travel and travel overseas. Avoid travel, travel overseas. Avoid travel. Avoid travel overseas, travel overseas, travel overseas. Avoid travel, travel overseas. Avoid traveling overseas, travel overseas. Avoid travel, travel overseas. Avoid travel, avoid travel. Avoid travel overseas. Avoid travel, travel overseas. Avoid travel, travel overseas. Avoid travel overseas. Avoid travel overseas. Avoid travel. Avoid travel overseas. Avoid travel, travel. Avoid travel. Avoid travel. Avoid travel, travel overseas. Avoid travel. Avoid travel, travel. Avoid travel. Avoid travel, travel overseas. Avoid travel. Avoid travel. Avoid travel,. Avoid travel, travel overseas. Avoid travel. Avoid travel. Avoid travel overseas. Avoid travel. Avoid travel. Avoid travel, travel. Avoid travel, travel, travel. Avoid travel. Avoid travel. Avoid travel. Avoid travel overseas. Avoid travel. Avoid travel. Avoid travel. Avoid travel, travel overseas travel. Avoid travel. Avoid travel, travel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   148  of    228. Loss: 1.7247220277786255.   Elapsed: 0:08:24.\n",
            "0: :D:D D:D:D D:D:D D:D D:D D:D D:D D:D D:D D:D D:D D:D:D D:D D:D D:D D:D D:D D:D D:D D:D:D D:D:D D:D D:D:D D:D D:D D:D D:D D:D:D D:D D:D:D D:D D:D D:D D:D D:D D:D D:. D:D:D:D D:D:D:D D:D D:D D:D D:D D:D D:D:D D:D D:D:D:D:D D:D D:D D:D:D:D:D:D:D:D:D:D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   149  of    228. Loss: 0.8066304922103882.   Elapsed: 0:08:29.\n",
            "0: ’                                                                                                                          ’ ’                                                                      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   150  of    228. Loss: 1.527496337890625.   Elapsed: 0:08:34.\n",
            "0: 19.1 3 1,19.2 4,19 5 4,1919 19?19 3.19 4,19 5,19,19 6,19 5,19 4.19 6,19,1919,19 5,19 19 19,19 619,19 5,191919 19,1919?19,19?19,19?191919 1919 1 219,19,191919,19 4,19 2?19191919,1919 19,1919,19?19,19 5 19 4 5,1919,1919,192719 619,19.191919,19?19,191919,19,19,19,19 619 19 9,191919,19191919,191919 619,1919,1919,1919 5,19,191919,19,1919,191919,19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   151  of    228. Loss: 0.9763796329498291.   Elapsed: 0:08:39.\n",
            "0: ???                                                                                                                          .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   152  of    228. Loss: 0.5237232446670532.   Elapsed: 0:08:42.\n",
            "0: ,,.. 4:29:29,29:29:29.29:29:29:29:2929:29:2929 292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929002929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929292929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   153  of    228. Loss: 1.4139362573623657.   Elapsed: 0:08:47.\n",
            "0: ,,. 4 4,5,000 4,500 5,000,500 4,500,600,500,600 4,000,600,600 6,600 6,600 5,600,600 5,600 6,600,600 6,600 6,600 6,600 6,600 6,600,600 6,600,600 6600 6,600 6600,600 6,600 6600 6,600 6,600 6600 6,600 6600,600 6600,600 6600 6600,600 6600 6,600 6600 6,600 6600600 68600 6600,600 6600 6600 6600 6600 6600,600,600 6600,600,600 6800,600 6600,600 6600 6800 6600600600 6600 6600 6600,600,600 6600 6600 6600600 6600,600,600 6600600 6600 6600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   154  of    228. Loss: 0.629200279712677.   Elapsed: 0:08:52.\n",
            "0: , in brief:, text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat: chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   155  of    228. Loss: 1.2991657257080078.   Elapsed: 0:08:57.\n",
            "0: NGL JNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNG,NGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNGNG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   156  of    228. Loss: 0.6977581977844238.   Elapsed: 0:09:02.\n",
            "0: ,? The correct text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatTalk chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   157  of    228. Loss: 1.1621289253234863.   Elapsed: 0:09:06.\n",
            "0: ?In brief query?In brief query?In brief query?In brief query?In brief query?In brief query?In brief query.In brief query.In brief query.In brief query.In brief query.In brief query.In brief query.In brief query.In brief query.In brief query.In brief query.In brief query?In brief query.In brief query.In brief query.In brief query.In brief query.In brief query query query.In brief query.In brief query.In brief query.In brief query?In brief query.In brief query.In brief query.Please query query.In brief query query.In brief query.In brief: brief:In brief: brief:In brief: in brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief: brief:In brief:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   158  of    228. Loss: 2.411778688430786.   Elapsed: 0:09:11.\n",
            "0: ....     You can use any type of text chat with text chat with text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chatHello chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   159  of    228. Loss: 0.48946595191955566.   Elapsed: 0:09:16.\n",
            "0: 5 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   160  of    228. Loss: 0.34109675884246826.   Elapsed: 0:09:21.\n",
            "0: ?! What do you do now? What do you do now do now? What do you do now do now? What do now do now, but what do now now do now, what do now do now do now do do now, what do now do now do now?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   161  of    228. Loss: 0.4448743462562561.   Elapsed: 0:09:23.\n",
            "0: :D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:Thats help:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:D:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   162  of    228. Loss: 0.6724775433540344.   Elapsed: 0:09:28.\n",
            "0: ...???,?,?,???,?,?,?,?,?,?,?,?,?,?,??,??,??,,?,?,?,,???,,,,?,?,?,?,?,,?,?,,?,?,,?,?,,?,?,,,??,,,,?,,,,,,,,,,,,,,,,,??,?,?,,,,?,?,,,?,?,?,,,??,,,,?,?,?,,?,?,,?,,,,?,,,,,,,,?,,,,,,,,,,,,,,?,,,,?,,,,,,,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   163  of    228. Loss: 1.0129380226135254.   Elapsed: 0:09:33.\n",
            "0: ? I mean it as a text chat chat with someone but it really doesn't take any time for them to chat with you that text chat is just so much more important that text chat with text chat with someone is important like text chat chat with someone who text chat with someone that text chat with someone with text chat with someone important text chat with someone important chat with someone important chat chat with someone important chat with someone important text chat with someone important chat with someone important chat with someone important chat with someone important chat with someone important chat with someone important chat with someone important chat with someone important chat with someone important chat with someone important chat chat?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   164  of    228. Loss: 2.178694009780884.   Elapsed: 0:09:36.\n",
            "0: ,. 2 3 4?,19???19?,2 3??2 3,19,19?2,19,19,19?19,19,19,19,19??19,19,19?19,19?19,19?19?19?19,19?19?19,1919,19,19,19?19?19?19,19?19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,,19,19?19?19,19,19,19?19,19,19,19?19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   165  of    228. Loss: 0.9915733337402344.   Elapsed: 0:09:41.\n",
            "0: ?? I do understand your query but what does it mean? Did you video or text chat with a local doctor? I do understand your query but video or text chat with a local doctor? Did you video or text chat with a local doctor? Did video or text chat with a local doctor or text chat with a local doctor or text chat with someone that is doing video or text chat with a doctor?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   166  of    228. Loss: 0.8726711869239807.   Elapsed: 0:09:44.\n",
            "0: ??Possible to video or text chat with me? Also what is your query? I am still trying to get you answered or text chat with me to video or text chat with me or text chat with me?Hi there my query would be very nice to video or text chat with you or text chat with me to video chat with me or text chat with me or text chat with me to video or text chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   167  of    228. Loss: 0.9493396282196045.   Elapsed: 0:09:46.\n",
            "0: ............ I have no idea what that is.. Do I know what am am I doing right now? Any further questions? A video or text chat with your doctor? If you do have video or text chat with a doctor in the hospital or text chat with yourself?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   168  of    228. Loss: 1.2294409275054932.   Elapsed: 0:09:49.\n",
            "0: ,  Welcome to the chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   169  of    228. Loss: 1.1008938550949097.   Elapsed: 0:09:54.\n",
            "0: .. 2 3 3.3. 3,3 3 4.3 4,3 4,3,3,3 4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   170  of    228. Loss: 0.8471004366874695.   Elapsed: 0:09:59.\n",
            "0: ,  Don't worry about it, you should stay home from the community with good health and or take a pill for no reason for no reason so the community might not be happy with the health? Don t worry about it. Don t worry about the community\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   171  of    228. Loss: 1.065399408340454.   Elapsed: 0:10:01.\n",
            "0: Can this video be a video or text chat with me? Can someone video chat with me text chat with me? Can someone video chat with me chat with me chat with me? Can someone video chat with me text chat with me text chat with me chat with me chat with me chat chat with me chat chat chat with me chat with chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat: chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   172  of    228. Loss: 1.3875178098678589.   Elapsed: 0:10:05.\n",
            "0: ,!...,,.,, but it does mean something is true for once, right now does it not? I am concerned that the first time or text chat with text chat does not mean text chat does mean text chat, text chat is not necessarily chat with text chat in text chat chat, text chat with text chat is not as chat with text chat chat chat with text chat chat with text chat is not chat with text chat chat with text chat chat, text chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat Deboring chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   173  of    228. Loss: 0.38440045714378357.   Elapsed: 0:10:10.\n",
            "0: ??????? is that because she is a mother or because it is a good idea that you have to give someone specific advice to someone specific to give advice. advice on how to give specific advice on how to give specific advice. advice.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   174  of    228. Loss: 0.5274903178215027.   Elapsed: 0:10:12.\n",
            "0: ,.....???????,.,.,,!,.,....,.,..,,,.,,.,,,,,.,,,,,,?,,,,,,,,,,.,,,,,.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   175  of    228. Loss: 0.9533523321151733.   Elapsed: 0:10:14.\n",
            "0: , as do it. There it is now a text chat with me about the text chat chat and text chat with me, or text chat with me and text chat with me, text chat with me and text chat with me and text chat with me and text chat with me and text chat with me and text chat chat with me and text chat chat with me and text chat chat with me and text chat with me and text chat chat chat with me and text chat chat with me and text chat chat with me and text chat with me and text chat with me and text chat chat with me and text chat chat with me and chat chat launcer chat chat with me and chat chat chat with me is now chat with me and text chat with me. I chat with me. The text chat chat with me chat chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   176  of    228. Loss: 0.7791536450386047.   Elapsed: 0:10:19.\n",
            "0: ?, to be exact. A week or two on the internet and on the phone so. A week on the bus. A week or two on the bus and take a week. Two days or so at home. A week or two at home. A week or two on the bus and then a week later a week or so at the bus and take a half day to do something or text chat with him to your text chat with someone or text chat with your text chat with someone and text chat with your text chat with someone or text chat with someone in contact with someone or text chat with someone.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   177  of    228. Loss: 1.3150253295898438.   Elapsed: 0:10:22.\n",
            "0: ?........?.....??.. and my opinion is that it hurts the feelings if the body aches and feels more tender, so scratchy scratchy as a scratchy scratchy body aches and feel more tender as a scratchy body aches and feel more tender as a scratchy body aches and feel like this scratchy body aches. This body aches are really scratchy and scratchy and scratchy body aches and itchy itch is almost a bad thing, so I have to scratch scratch scratchy scratchy scratchy body aches and body aches for deficienolator and scratchy scratchy to scratchy scratchy scratchy body aches are no one of the muscle aches in the body aches for one does the body aches are also scratchy body aches and the body aches and no pain for a thing on the body aches and no scratchy scratchy body aches and\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   178  of    228. Loss: 0.7648640275001526.   Elapsed: 0:10:27.\n",
            "0: ,?,,,,.,,,,.,,.,.,,,,,,,,,.,,,,,,,,,.,,,,.,,,,,,,,.,,,,,?,,,,,.,,,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,.,,,,they are some country, and the country, for your query is such as well regarded as countries, for your first and so it is now, or just not in the rest, a country is, but it is just the answer. The country, after they would you will be happy to live in your query.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   179  of    228. Loss: 1.1577097177505493.   Elapsed: 0:10:32.\n",
            "0: ,?...??? A video or text chat with your doctor as to your doctor?In brief: history: history.com, history: history:History: history: history: history.com.com, history.com.com history: history: history: history: history:history: history history history: history. history: history.com, history: history: history history. history: history history: history: history, history:  If history: history: history.com history:history:history:history.com.com history: history history history:history.com, history: history history: historyHistory: history: history:history.com/history.com history: history: history history.com history: history: history history: history: history: history: history:history history: history:history.com/history: history.com history: history.com/history:history history: history:history:history.com/history: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   180  of    228. Loss: 0.9104816317558289.   Elapsed: 0:10:36.\n",
            "0: ,,..  Call in your question in your query. What is the query on the query. I will give you the query query details, which query on the query. How is your query on your query is on the query. What is the query query on the query on your query. What query query is on your query on query query. How is the query on your query on the query query. How does your query query query get query on query queries of query.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   181  of    228. Loss: 0.6290287375450134.   Elapsed: 0:10:39.\n",
            "0: .    Also is it safe to use a doctor or pharmacist if you have any self help clinic or pharmacist.  Your use of antibiotics and antidepressants or antidepressants should be treated with medications or drugs, or just use the best option of medicine.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   182  of    228. Loss: 0.635334849357605.   Elapsed: 0:10:41.\n",
            "0: ,,,,..,.,.,???,, '., '.,,,?,,,,,,,,,?,,,,,,?,,,,,,,,,,,?,,,,,,,,.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   183  of    228. Loss: 0.7223638892173767.   Elapsed: 0:10:44.\n",
            "0: ...? Cough for the rest of the night cough for the rest of the night cough cough cough cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the night cough for the rest of the rest of the night cough cough cough for the rest of the night cough for the rest of the night cough cough for the rest of the rest of the night cough for the rest of the nightYou like to cough for the rest of the day cough for the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the rest of the night cough cough for the rest of the rest of the night cough, cough?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   184  of    228. Loss: 0.8274202942848206.   Elapsed: 0:10:48.\n",
            "0: ??.. I am not sure that will work for me as well as it will for my last 2.5 year old daughter as a 6 year old. This should probably do it as a regular person though. Please do this for me as a regular thing and you just have to get started on the 2.5 year old as a doctor with two kids and 2 kids with two kids will get them together with you quickly as a GP?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   185  of    228. Loss: 1.086040735244751.   Elapsed: 0:10:51.\n",
            "0: If you like to play video or text chat with me go to chat with me? I have tons of video to video chat with others. I do video chat with other people online all the time. Usually take video or text chat to video chat with me as a good time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   186  of    228. Loss: 0.9210882186889648.   Elapsed: 0:10:53.\n",
            "0: ?,..,...????,??,?,,?,,?,,,,,?,.,,,,??,,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,,.??,,,?,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,,,,,?,,,,,,,,,,,,?,,,,glomeration,,,,,,?,,,,,,,,,?,,,,,,,,,,,,,,,,,?,,,,,?,,,,?,?,,,,,,,,,,?,,,,,,?,,,,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   187  of    228. Loss: 2.8605589866638184.   Elapsed: 0:10:58.\n",
            "0: . 3 5 7,000,000, 000,000,000,000,000,000, 000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, mathematcalcmls.000,000,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls,mls\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   188  of    228. Loss: 1.0790047645568848.   Elapsed: 0:11:03.\n",
            "0: .?..,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,?,,,?,,,?,,,,,,,,,?,,,,,,,,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,,,?,,,,,,?,,,,,,, CitizD,,,,,,,,,,,,,?,,?,,,,,,,,,?,,,?,,,,,?,,,,,,,,,,,,?,?,,,,,,,,,,?,,,,,,?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   189  of    228. Loss: 2.815819025039673.   Elapsed: 0:11:08.\n",
            "0: ...  Would you like to video chat with me? I will video chat with you and you will video chat with me? I will video chat with me? I have video chat chat with you in video chat with me. Let me video chat with you or text chat with me as I text chat with you chat chat with me chat with me and text chat with me chat with me or text chat with me chat chat chat with me or text chat with me chat with me chat with me chat chat with me chat chat with me chat chat chat with me chat chat with me chat with me chat chat chat with me chat chat!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   190  of    228. Loss: 0.8163330554962158.   Elapsed: 0:11:11.\n",
            "0: ,,,....,..,..,,,, and, and the community does not have any ability to rule out any group, community, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, group of people, and group of people.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   191  of    228. Loss: 0.4991101622581482.   Elapsed: 0:11:14.\n",
            "0: ,,,,,,,, but, but, but that is so much effort, but so much time. and also much time. Is not really much to worry about getting time. Let him, not worry, but worry, but not worried, and not worried?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   192  of    228. Loss: 0.88072669506073.   Elapsed: 0:11:16.\n",
            "0: .   Don't try to be happy at least until the contract is contract    self contract with contract contract  Would you like to video or text chat with me? Also it is contract    Do not text chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   193  of    228. Loss: 0.8244346976280212.   Elapsed: 0:11:18.\n",
            "0: ? but but but but the USA is so nice. The US is so big it makes me sick that all of them have a sore throat and a sore throat and the US is so sore as the only one of them has the heart as you have been sore as well as the rest of the world is so sore you need to be sore as well as possible to be sore as you are in the USA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   194  of    228. Loss: 0.6822522282600403.   Elapsed: 0:11:21.\n",
            "0: In brief:  I am a private person and I have been doing yoga for over a week now.In brief:  Would you like to video or text chat with me? Also consider video or text chat with me?  You should video chat with me or text chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   195  of    228. Loss: 0.9976978302001953.   Elapsed: 0:11:23.\n",
            "0: ,,,,.,,,,?,?,.,,,,?,,,?,,??,,,?,,,,,?,,.,,,.,,?,,,?,,,?,,?,,,?,,,,,,?,,,,,,,,,,,,,,?,?,,,,,,,,,,,,,,,,,,,,,,,?,?,,,,,,,,,,,,,?,,,,,,,,,,,,,?,?,,,?,?,,?,,,?,,,,,?,,?,,,,,,?,?,,?,,,,,,??,,,,,,,,?,,?,,,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   196  of    228. Loss: 0.9430318474769592.   Elapsed: 0:11:28.\n",
            "0: ..  Your point is that it's weird for him to video chat with me about video chat with me??I do video chat with him because he has video chat with me now. He does video chat with me now. He has video chat with me now. He has video chat with me. He has video chat with me now. He has video chat with me now. He has video chat with me now. He has video chat with me now, but does video chat with me now, he has video chat with me now. He does video chat with me now. He text chat with me sometimes chat with meIn brief:  When video chat with me:  Also video or text chat with me text chat with me, text chat with me now. He has video chat with me?I don't chat with me? He is still does not text chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   197  of    228. Loss: 1.485897421836853.   Elapsed: 0:11:32.\n",
            "0: , not, in the history of the world or history of history of the world. In brief:     It was on the history book in history book in the history history of the world history book for history book history book history book.      Is it possible to have history history book history book history book history book history book history history book history book history book history book history book history book history book history book history book history book history book history book history history book history book history book history book history history book history book history history book history book history book history book history book history book history book history book Citiza book history history book history book history history book history history book history history book history book history book book history book history book history book history book history book history book history history book history book history book history history history book history book history book history book history book history history book history book history book history book history book history book history book history book history book\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   198  of    228. Loss: 0.5572049021720886.   Elapsed: 0:11:37.\n",
            "0: ,.......?...?.??......?.?.?.?.??...?????? I took a lot of time getting to stay home during the festival and stay home during the festival. I took a lot of time getting to stay home during the festival. I took a lot of time getting to get to stay home during the festival but also took a lot of time getting to get my stuff yesterday and today. My stuff is in the festival. I took a lot of time getting my stuffs this morning and my stuff today I take it seems like there seems like it seems like there are tonsils yesterday but yesterday I take a lot of it is not got a lot. I took a lot of stuff but today I take it seems like tonsils, plenty to the thing that morning and tonsils take it was just take it just take it seems that this morning for me back to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   199  of    228. Loss: 0.7565532922744751.   Elapsed: 0:11:42.\n",
            "0: ,,.,,.,.,,,,. I don't have enough time or any idea how time is time is called so much time that video time is short time for so little time so time. I got time yesterday so get some sleep. What time is time for video time is short time for video time is the video time should video time is short time to video time is short time for short time, short time for video or text chat chat chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   200  of    228. Loss: 0.6099117398262024.   Elapsed: 0:11:45.\n",
            "0: ,...  Stay safe. Stay safe and stay safe. Stay safe Stay safe and stay safe and stay safe and stay safe and stay safe and stay safe! Stay safe and stay safe and stay safe and stay safe and stay safe and stay safe stay safe and stay safe and stay safe and stay safe and stay safe and stay safe and stay safe Stay safe and stay safe and stay safe and stay safe stay safe and stay safe with me.Stay safe and stay safe and stay safe as well as possible as possible, stay safe and stay safe as possible if possible. Stay safe and stay safe and stay safe as possible, and\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   201  of    228. Loss: 2.857764482498169.   Elapsed: 0:11:48.\n",
            "0: ?..........??, and some people also call them the internet. So there is hope for us to be cured somehow. So help us spread the virus which is the virus that we virus of the virus. So as an antibiotic we are just going to be cured somehow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   202  of    228. Loss: 1.2296026945114136.   Elapsed: 0:11:50.\n",
            "0: !!! I am a good bookie to book a book with a good book contract and book contract contract contract that contract contract book contract contract contract contract contract book contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract mathematations contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract contract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   203  of    228. Loss: 0.8898706436157227.   Elapsed: 0:11:55.\n",
            "0: ,,.,,,.,,,,,,,....,.,,,,,,,,,. I took a nap on the bus yesterday, and woke up this morning, woke up today morning, woke up yesterday I took a nap, woke up last night woke up yesterday morning, woke up yesterday morning, woke up this morning today, woke up yesterday morning yesterday morning, woke up yesterday, woke up yesterday morning, woke up yesterday morning I took a nap yesterday and woke up yesterday yesterday morning, woke up yesterday morning, woke up yesterday morning, woke up yesterday morning yesterday morning morning Yesterday yesterday, yesterday morning yesterday morning, I woke up yesterday morning yesterday morning yesterday morning yesterday morning, Yesterday morning yesterday morning I woke up yesterday morning yesterday morning yesterday morning, yesterday morning, yesterday morning, woke up yesterday morning yesterday morning, yesterday morning I woke up yesterday evening, yesterday morning Yesterday morning, yesterday morning, yesterday morning morning?I woke up yesterday morning I woke\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   204  of    228. Loss: 0.3209877014160156.   Elapsed: 0:12:00.\n",
            "0: .. 3.5 4.0 4.0 4.0. 3.5 4.0 4.0 4.5 4.5 4.5 6.0 4.5 4.5 6.5 5.5 4.5 6.5 4.5 4.5 4.5 6.5 4.5 6.5 6.5 6.5 5 5.5 6.5 6.5 6.5 4.5 6.5 6,5 6.5 6.5 5.5 6.5 5 6.5 6.5 6 6.5 4.5 6I have 4,5 6.5 7,6 6 6.5 6 4 5 6 4.5 6.5 6:3 6 6 6.5 6 6,5 6.5 6 6.5 6.5 6.5 6 6.5 6.5 6 6.6 6,6 6 6 6.5 6 6 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   205  of    228. Loss: 0.7840462923049927.   Elapsed: 0:12:05.\n",
            "0: ?   Would you like to video or text chat with me?    I have video or text chat with me?  If so please text chat with me?     It is good with me? If I have video or text chat with me or text chat with me?        Would you like to video or text chat with me?                 I have text chat    Would you like to video or text chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   206  of    228. Loss: 1.0007352828979492.   Elapsed: 0:12:08.\n",
            "0: ? I do this but video chat chat with me is so much better I text chat with chat chat with text chat with chat chat with me when text chat with me chat with me chat with me chat with me? I do text chat with me text chat with me text chat with me text chat with me chat with me chat with me chat with me text chat with me text chat with me chat with me chat with me text chat with me chat with me text chat with me text chat with me text chat with me text chat with me chat with me text chat with me chat with me text chat with me text chat with me text chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   207  of    228. Loss: 0.8925748467445374.   Elapsed: 0:12:12.\n",
            "0: , or a picture or text chat with me? Any way to video chat with me? I got a text chat with someone for you to video chat with me yesterday. I have a text chat with him that was in touch with him yesterday.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   208  of    228. Loss: 0.6765409111976624.   Elapsed: 0:12:14.\n",
            "0: :                                                                                                                            � The                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   209  of    228. Loss: 0.86378014087677.   Elapsed: 0:12:19.\n",
            "0: :      :                                                                                                                        I NSA                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   210  of    228. Loss: 0.868241012096405.   Elapsed: 0:12:23.\n",
            "0: ?  Is it just me?   Is it just me?                                                                                                                 It was like it or                                                                   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   211  of    228. Loss: 0.6070681810379028.   Elapsed: 0:12:28.\n",
            "0: ,,,,,,,,,,,,,, or,. or, or. As this site reports what I report. It is more. As one of the best in the community this site reports which are all over 20,19. I suggest using. But it has not been effective. A good site since now. As it says not one, but that it now it has been more than 20,19,19. According to some people like to the community. All that.In brief: C, or short:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   212  of    228. Loss: 0.93625408411026.   Elapsed: 0:12:32.\n",
            "0: .  You can even make text chat with me3.      You can text chat with me3.      You can text chat with me3.     I can text chat with me3.       You can text chat with me 3.                                                                is a social contact with me                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   213  of    228. Loss: 0.7800231575965881.   Elapsed: 0:12:36.\n",
            "0: ,,                                                                                                                            You?                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   214  of    228. Loss: 0.6127365827560425.   Elapsed: 0:12:41.\n",
            "0: ?? Is that a problem? I am confused on what the difference between this and a lot of the other stuff that goes on now and a lot of other stuff going on now but some stuff going on in a lot of stuff and stuff going on now that a lot of stuff is going on now and stuff going on right now is going on in this stuff now and the current stuff going on now and stuff that is going on now and stuff. What should I take now?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   215  of    228. Loss: 0.6846733689308167.   Elapsed: 0:12:44.\n",
            "0: ??... How can you call someone just a video with no video or text chat with someone with just a video or text chat with someone with a video or text chat with a text chat with a text chat with a text chat with someone with a text chat with someone with a text chat with a text chat with someone with a text chat with a text chat with someone with a text chat with someone with a text chat with someone with a text chat with someone with a text chat with someone with a text chat with someone with someone with a text chat with someone with a text chat with someone with a chat with someone with someone with a?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   216  of    228. Loss: 1.5833183526992798.   Elapsed: 0:12:48.\n",
            "0: ???I have no experience with this experience it's a video or text chat with me’s me?What does it mean for this one? What is it mean for me? What has it mean for me? Is it like me?In brief:                                                                        ...                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   217  of    228. Loss: 0.5853567719459534.   Elapsed: 0:12:52.\n",
            "0: . 10. 0,0,100,100,100,000,100,00,000,000,000,000,000,000,000,000,000,000,000,000,000,00,000,000,000,000,000,00,000,00,000,000,000,000,00,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00,000,000,000,000,000,00,000,000,000,000,,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   218  of    228. Loss: 1.1687662601470947.   Elapsed: 0:12:57.\n",
            "0: ??? Did you even take the picture?In brief this time since you have a history with someone with the NSA, your history with someone with the NSA and NSA. I have not done so since I have the NSA history with NSA but if you are not sure your history with NSA they will come back to you with the NSA history of NSA NSA that is not true.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   219  of    228. Loss: 0.8186649680137634.   Elapsed: 0:13:00.\n",
            "0: . I am a poor doctor, so here is my doctor's opinion. If you need further information, I can assist you on the right kind of care for you in the community.If you have no experience with such as I do, please video or text chat with me or text chat with me or text chat with me with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   220  of    228. Loss: 1.8333710432052612.   Elapsed: 0:13:02.\n",
            "0: ,,......,,, and then get out and breathe and feel better as you breathe it goes on... you have had minor symptoms, and still breathing and breath and breathe like a breath but still have minor symptoms and breath and breath, breathe it is a virus and breath and breathe it is a fever. There is so much that breathe it goes into your lungs. Try doing some exercise and do some breathing exercises for now. Keep yourself healthy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   221  of    228. Loss: 0.4226836562156677.   Elapsed: 0:13:05.\n",
            "0: .                                                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   222  of    228. Loss: 1.1519110202789307.   Elapsed: 0:13:07.\n",
            "0: .'Sore throat and fever? What illness? What illness is this for someone with such low fever and throat, low appetite?I do not remember an illness that was such a symptom that such as an asthma, or a sick. There are so many ways they can get it done without having a fever, such as being on food so many medicines that cannot be done at home, however, you might be able to use proper testing or even call your doctor, or any other doctor, that has a good health service in that area with such a better health services. Also, it helps in an emergency, such as taking medication and working up of food, exercise, drink plenty of fluids and proper fluids, sleep, sleep, such as well, but in shortness of breath, washing, such as I do not really want to eat, such as a healthy, food or breath and exercise, lightheadedness of breath, sleep, daily, daily, warm water or body ache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   223  of    228. Loss: 0.7567738890647888.   Elapsed: 0:13:12.\n",
            "0: ,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,, or, I have been to school and have traveled all over South Africa and now. I feel safer now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   224  of    228. Loss: 1.8750003576278687.   Elapsed: 0:13:14.\n",
            "0: . 6:19.5? 6:19.5.1:00:0:0:0:10.5:00:0:0:0:5:0:0:0:0:00:0:0:0:10:0:0:0:0:0: 0:0:0:0:20:3:20:0:0:0:0:0:20:0:0:0:3:0:0:0:0:0:0:0:0:0:5:0:0:0:0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   225  of    228. Loss: 0.7552502155303955.   Elapsed: 0:13:18.\n",
            "0: .....  Would you like to video or text chat with me?  I can video chat with you further or text chat with someone more to video chat with me further.  Would you like to video or text chat with me more?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   226  of    228. Loss: 1.8679051399230957.   Elapsed: 0:13:20.\n",
            "0: 3rd one got reviewed with 10 seconds left I got reviewed with 10 seconds left I got reviewed with 10 seconds left and got reviewed with 10 seconds left I got reviewed with 10 seconds left and got reviewed with 10 seconds left I got reviewed with 10 seconds left and got reviewed with 10 seconds left I got reviewed with 10 seconds left I got reviewed with 10 seconds left I got reviewed with 10 seconds left and reviewed with 10 seconds left I got reviewed with 10 seconds left I got reviewed with 10 seconds left and got reviewed with 10 seconds left I got reviewed with 10 seconds left I got reviewed with 10 seconds left I got reviewed with 10 seconds left was reviewed with 10 seconds left and get evaluated with 10 seconds left for the video with 10 seconds left and got reviewed with 10 seconds left with 10 seconds left and reviewed with 10 seconds left because I got reviewed with 10 seconds left with 10 seconds left right left left with 10 seconds left with 10 seconds left with 10 seconds left right left with 10 seconds left with 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   227  of    228. Loss: 0.7384771108627319.   Elapsed: 0:13:25.\n",
            "0: ??                                                                \n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epoch took: 0:13:27\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     1  of    228. Loss: 0.8113510608673096.   Elapsed: 0:00:01.\n",
            "0: , 3 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3?3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     2  of    228. Loss: 0.9589663743972778.   Elapsed: 0:00:06.\n",
            "0: ,    or like to video chat with me or text chat with me                                                                                                             ? It                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     3  of    228. Loss: 0.9450531005859375.   Elapsed: 0:00:11.\n",
            "0: ??    What is going on here?? Is there some weird rule in the   Not this   I cannot understand you ? How can I understand you and what is going on here?   You do not have to take it with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     4  of    228. Loss: 0.8483908772468567.   Elapsed: 0:00:13.\n",
            "0: ,   The most people I interact with in touch with are also with children on a forum for the children to chat with me? What should I do:’t do: Do: interact with children, young kids, or young kids, adults, children, or children on a forum like social media?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of    228. Loss: 1.0206323862075806.   Elapsed: 0:00:15.\n",
            "0: , lt lt 3 lt 3 lt 3 lt 3 lt 3 lt 3 Lt 3 lt 3 lt 3 lt 3 lt 3 lt 3 lt 3 lt 3 lt3 lt3 lt 3 lt 3 lt3 lt 3 lt 3 lt3 lt3 lt 3 lt 3 3 lt3 lt 3 lt 3 lt3 lt 3 lt 3 lt3 lt 3 lt 3 lt 3 lt3 lt 3 lt 3 lt 3 lt 3 lt 3s3 lt 3,3 lt 3 lt 3 lt3 lt3 lt3 lt3 lt3 3l3 lt 3 lt3 3 lt 3,3 3 lt3 lt3l3 3.4 lt 3. lt 3,3 lt3 is not3?3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     6  of    228. Loss: 0.5970526337623596.   Elapsed: 0:00:20.\n",
            "0: ?                                                  ’    ’           ?      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     7  of    228. Loss: 0.6609901189804077.   Elapsed: 0:00:23.\n",
            "0: ????? are the few few that are still around I take issue with some restrictions on text chat chat restrictions. This is one of them. Hi I took a few minutes last night and now I got my ban yesterday as a few hours now.Hi I take issue with text chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on’t on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions on chat restrictions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     8  of    228. Loss: 0.9790723919868469.   Elapsed: 0:00:28.\n",
            "0: In brief this video is my experience with pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic.  You can not really rule out pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic, pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic, pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic pandemic of pandemic pandemic pandemic pandemic.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     9  of    228. Loss: 0.7455563545227051.   Elapsed: 0:00:31.\n",
            "0: , I got my arm back yesterday and now have not put it back on in over a week and a half now and still have not put it back on my body yet. I put it back on again this time today and got my arm back yesterday and have put on my body yesterday. I have put it back on now as well as I put it on my body yesterday and now still have not put it back on again this time and it is now gone. I try. The body was in my arms today but now it just takes some time to put it back on again and is now the body but today it just feels safer, my arm and it got back on again this time. Thank you for your doctor on my arm muscles. I put it on now. Keep us all that your chest up with your query.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of    228. Loss: 0.6226917505264282.   Elapsed: 0:00:35.\n",
            "0: ??In brief:                                                                                                                         �?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    11  of    228. Loss: 1.1695358753204346.   Elapsed: 0:00:39.\n",
            "0: ???In brief: The text chat with me:  You have answered any questions here :  The chat with me:  My question is, if I answer any:  You do:   Is it possible for me to do:    the text chat with me, or text chat with me? How can I be completely sure that it is a chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    12  of    228. Loss: 1.28946053981781.   Elapsed: 0:00:42.\n",
            "0: ,...??????????,?,.,?,,,??,?,?,?,?,,?,?,?,,,,??,?,,?,,,.,,?,,,.,,,,,,,,,,?,,,,,?,,,,,,,,,?,,,,,,,,,,,,,,,,,,,,,,,?,?,,,,,,,?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,?,,,?,,,,,,,,,,,?,,,,,,,,,,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    13  of    228. Loss: 0.6958006024360657.   Elapsed: 0:00:46.\n",
            "0: ,     is the best way.                                                                                                                    I,                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    14  of    228. Loss: 0.3168846666812897.   Elapsed: 0:00:51.\n",
            "0: In brief:                                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    15  of    228. Loss: 0.5883113741874695.   Elapsed: 0:00:53.\n",
            "0: ,  Self Regards, SOBs and SOBs and WOBs SOBs like to visit family and friends in the community so better to support others by doing so, even though it is unlikely that they will be happy to take up the time, or any other way to help you take up the time to help you.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    16  of    228. Loss: 2.424173593521118.   Elapsed: 0:00:56.\n",
            "0: ,.,.,,.,,,.,,,,,, and in the beginning there were, except there was no clear and very clear evidence of that since the present evidence, and we are not to say anything as there was no clear and very clear data until the last few weeks in this post I have not been in contact with him?. I have not been in contact with him so how do we know that you do you do not have direct contact with him or a local community to discuss in public chat with him?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    17  of    228. Loss: 0.4959152340888977.   Elapsed: 0:00:59.\n",
            "0: !!!.....!!     You have reviewed your query and reviewed your query with the query query.   You have reviewed your query and reviewed your query with the query.   Your query has reviewed your query.    There is a query query now available in your query query on   Would you like to video or text chat with me further?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    18  of    228. Loss: 1.5309466123580933.   Elapsed: 0:01:01.\n",
            "0: ?,???...?,?,,,?,,,,?,??,?,,?,,,?,,?,?,?,,?,?,,?,,?,?,?,,?,?,,?,?,?,,?,,?,,,??,,,?,?,,,,?,?,,,?,,?,,?,?,?,,?,,?,?,,?,?,?,?,?,?,?,?,,,?,,?,,,??,?,,,?,,,?,?,?,,?,?,?,,,,,?,,,,?,,,,?,?,,,?,?,,,,?,,?,,,??,?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    19  of    228. Loss: 0.71783846616745.   Elapsed: 0:01:06.\n",
            "0: ... and the internet has answered all your query in this query. Welcome to the internet. I have answered all the query in your query further and it is self isolate your query as the internet states that the query is in the data for query is not based on self isolate. The query is in the data for the query in question.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    20  of    228. Loss: 1.5979201793670654.   Elapsed: 0:01:08.\n",
            "0: ,,.......,..,...,,,,..,,,., and?,.,., and,, and, or,., and, or, unless they have a serious condition, they are just like any other type of disease like the flu, chills, headache, or cold?... or GERD or GERD, but there is not any known cause?... or is that just normal, even with all normal precautions or normal, normal cough, but not normal, nor very hot, or flu, or such, or chest x ray, if, or monitor all other common hygiene for fever, or clear your temperature, etc.In brief:D:  However, in this case, a cough, have had fever, chest x ray and chest x ray is not the temperature and monitor your temperature, chest x ray or X ray, or x ray. Follow your local testing and see doctor.In\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    21  of    228. Loss: 0.8004636764526367.   Elapsed: 0:01:13.\n",
            "0: . 6.1 10 20 6.1 10 7 10 7 7.1 6 5 10 7.1 6.2 11 7.2 10 7 7.1 6 3 6.3 6 5 10 7 6.3.3 7.5 7 7 7.1 3 6 10,7 2 10 7:4 5 6 6 7,7 2 6:5 4 6 4 6.5 6:5 6 6 7:6 6.5 7,6 6?5 6.5 6 6,3 6,3 6 3:5 7 7,7 7,5 6:7 7:3 6 3 6 7 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    22  of    228. Loss: 0.7161824703216553.   Elapsed: 0:01:17.\n",
            "0: ??                                             ?               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    23  of    228. Loss: 0.6475236415863037.   Elapsed: 0:01:19.\n",
            "0: DV dV                                          ’   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    24  of    228. Loss: 0.530016303062439.   Elapsed: 0:01:21.\n",
            "0: ,.........,  Stay home, stay home, stay home, stay home, stay home, stay home, stay home, and then go back home, and video or text chat with me?? If so I need to video chat with you, I can video or text chat with you, or text chat with you, or text chat with me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    25  of    228. Loss: 0.4782659411430359.   Elapsed: 0:01:24.\n",
            "0: ?? So where can I video or video chat with you about what it's like to do to do with it with it to do with it                                                                                                ,!                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    26  of    228. Loss: 0.6109449863433838.   Elapsed: 0:01:29.\n",
            "0: ?? Is that what you do? How am I supposed to interact with someone on this forum if the community community can interact with me on here on this forum but I am not interacting with someone on this forum but on this forum on this forum.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    27  of    228. Loss: 0.9329959154129028.   Elapsed: 0:01:31.\n",
            "0: ,,....,,,,,.,,.,,.,,.,,,,,,,,,.,,,,,.,,,,,,,,?,,.,,,,,.,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,., horizont.,,,,,,,,,,,,,,.,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,,,,,,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    28  of    228. Loss: 0.8418892025947571.   Elapsed: 0:01:35.\n",
            "0: ...  Self isolate yourself from someone who repeats repeats repeats repeats.  Would you like to video or text chat with me?   Stay home today and breathe deep. I hope you feel better.Regards, Adopt yourself and find yourself a way to interact with someone else in isolation if so so.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    29  of    228. Loss: 0.7161491513252258.   Elapsed: 0:01:38.\n",
            "0: ??   Isolate yourself, not only in this sub, you have to do:    Let the community know if                                                                                                   ,                                                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    30  of    228. Loss: 1.0137240886688232.   Elapsed: 0:01:43.\n",
            "0: .....!!!,.,...., is this where video chat with me, or text chat with me? I have to chat with you somehow. I have no idea where to video chat with you are really talking to you right now, I would like to video chat with you please?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    31  of    228. Loss: 0.7545797824859619.   Elapsed: 0:01:45.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-71ebb13d74a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                                     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                                     \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                     \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                                 )\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m             )\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m             )\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         )\n\u001b[1;32m   1124\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    955\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m                 )\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, prefix_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         )\n\u001b[1;32m    450\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, prefix_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upcast_and_reordered_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, prefix_mask, head_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_attn_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# Layer-wise attention scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myd0_JlcsWJY"
      },
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOlJwFuosWJY"
      },
      "source": [
        "loss_curves(df_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vVU3SIosWJY"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kZUklPnNaOr"
      },
      "source": [
        "model = model.to('cpu')\n",
        "rougeL = []\n",
        "rougeLsum = []\n",
        "rouge1 = []\n",
        "rouge2 = []\n",
        "\n",
        "for i, encoder_input in enumerate(test_dataset.input_ids[0:5]):\n",
        "    encoder_input = encoder_input[encoder_input != 50256]\n",
        "    encoder_input = torch.reshape(encoder_input, (1, -1))\n",
        "\n",
        "    predicted = model.generate(encoder_input, max_length=1000, min_length=50, return_dict=True)\n",
        "    \n",
        "    decoded_input = tokenizer.decode(encoder_input[0], skip_special_tokens=True)\n",
        "    decoded_prediction = tokenizer.decode(predicted[:, encoder_input.shape[-1]:][0], skip_special_tokens=True)\n",
        "    decoded_ground = tokenizer.decode(test_dataset.decoder_ids[i], skip_special_tokens=True)\n",
        "    \n",
        "    print(\"input: \")\n",
        "    print(decoded_input)\n",
        "    print(\"prediction: \")\n",
        "    print(decoded_prediction)\n",
        "    print(\"ground: \")\n",
        "    print(decoded_ground)\n",
        "    rouge_dict = rouge(decoded_prediction, decoded_ground)\n",
        "    rougeL.append(rouge_dict[\"rougeL\"])\n",
        "    rougeLsum.append(rouge_dict[\"rougeLsum\"])\n",
        "    rouge1.append(rouge_dict[\"rouge1\"])\n",
        "    rouge2.append(rouge_dict[\"rouge2\"])\n",
        "\n",
        "avg_rougeL_p = sum([rougeL[i].precision for i in range(len(rougeL))])/len(rougeL)\n",
        "avg_rougeL_r = sum([rougeL[i].recall for i in range(len(rougeL))])/len(rougeL)\n",
        "avg_rougeL_f = sum([rougeL[i].fmeasure for i in range(len(rougeL))])/len(rougeL)\n",
        "\n",
        "avg_rougeLsum_p = sum([rougeLsum[i].precision for i in range(len(rougeLsum))])/len(rougeLsum)\n",
        "avg_rougeLsum_r = sum([rougeLsum[i].recall for i in range(len(rougeLsum))])/len(rougeLsum)\n",
        "avg_rougeLsum_f = sum([rougeLsum[i].fmeasure for i in range(len(rougeLsum))])/len(rougeLsum)\n",
        "\n",
        "avg_rouge1_p = sum([rouge1[i].precision for i in range(len(rouge1))])/len(rouge1)\n",
        "avg_rouge1_r = sum([rouge1[i].recall for i in range(len(rouge1))])/len(rouge1)\n",
        "avg_rouge1_f = sum([rouge1[i].fmeasure for i in range(len(rouge1))])/len(rouge1)\n",
        "\n",
        "avg_rouge2_p = sum([rouge2[i].precision for i in range(len(rouge2))])/len(rouge2)\n",
        "avg_rouge2_r = sum([rouge2[i].recall for i in range(len(rouge2))])/len(rouge2)\n",
        "avg_rouge2_f = sum([rouge2[i].fmeasure for i in range(len(rouge2))])/len(rouge2)\n",
        "\n",
        "print(\"RougeL - precision: \" + str(avg_rougeL_p) + \", recall: \" + str(avg_rougeL_r) + \", fmeasure: \" + str(avg_rougeL_f))\n",
        "print(\"RougeLsum - precision: \" + str(avg_rougeLsum_r) + \", recall: \" + str(avg_rougeLsum_r) + \", fmeasure: \" + str(avg_rougeLsum_f))\n",
        "print(\"Rouge1 - precision: \" + str(avg_rouge1_p) + \", recall: \" + str(avg_rouge1_r) + \", fmeasure: \" + str(avg_rouge1_f))\n",
        "print(\"Rouge2 - precision: \" + str(avg_rouge2_p) + \", recall: \" + str(avg_rouge2_r) + \", fmeasure: \" + str(avg_rouge2_f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEi_A-DbsWJZ"
      },
      "source": [
        "model = model.to('cpu')\n",
        "step = 0\n",
        "while True:\n",
        "    context = input(\">> User:\")\n",
        "    if context == \"I am done talking\":\n",
        "      print(\"DialoGPT: Ok bye bye\")\n",
        "      print(\"How well did I answer the question? (See the five-point scale in the readme.)\")\n",
        "      input(\"Answer: \")\n",
        "      print(\"Thanks! How fluent were my answers? (See the five-point scale in the readme.)\")\n",
        "      input(\"Answer: \")\n",
        "      break\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids1 = tokenizer(context, return_tensors='pt').input_ids\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids1 = torch.cat([chat_history_ids1, new_user_input_ids1], dim=-1) if step > 0 else new_user_input_ids1\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids1 = model.generate(bot_input_ids1, max_length=1000, min_length=50, return_dict=True)\n",
        "    step += 1\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids1[:, bot_input_ids1.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr4t-i60sWJZ"
      },
      "source": [
        "print(model.device)\n",
        "\n",
        "print(bot_input_ids1.device)\n",
        "\n",
        "# Can't be different"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmWVVTLCsWJa"
      },
      "source": [
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}